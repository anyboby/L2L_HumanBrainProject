
@article{wangLearningReinforcementLearn2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.05763},
  primaryClass = {cs, stat},
  title = {Learning to Reinforcement Learn},
  abstract = {In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience.},
  language = {en},
  journal = {arXiv:1611.05763 [cs, stat]},
  author = {Wang, Jane X. and {Kurth-Nelson}, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z. and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},
  month = nov,
  year = {2016},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/mo/Zotero/storage/V9KHIKLI/Wang et al. - 2016 - Learning to reinforcement learn.pdf}
}

@article{harbInvestigatingRecurrenceEligibility,
  title = {Investigating {{Recurrence}} and {{Eligibility Traces}} in {{Deep Q}}-{{Networks}}},
  abstract = {Eligibility traces in reinforcement learning are used as a bias-variance trade-off and can often speed up training time by propagating knowledge back over time-steps in a single update. We investigate the use of eligibility traces in combination with recurrent networks in the Atari domain. We illustrate the benefits of both recurrent nets and eligibility traces in some Atari games, and highlight also the importance of the optimization used in the training.},
  language = {en},
  author = {Harb, Jean and Precup, Doina},
  pages = {8},
  file = {/home/mo/Zotero/storage/HC55894S/Harb and Precup - Investigating Recurrence and Eligibility Traces in.pdf}
}

@article{bellecBiologicallyInspiredAlternatives,
  title = {Biologically Inspired Alternatives to Backpropagation through Time for Learning in Recurrent Neural Nets},
  abstract = {The way how recurrently connected networks of spiking neurons in the brain acquire powerful information processing capabilities through learning has remained a mystery. This lack of understanding is linked to a lack of learning algorithms for recurrent networks of spiking neurons (RSNNs) that are both functionally powerful and can be implemented by known biological mechanisms. Since RSNNs are simultaneously a primary target for implementations of brain-inspired circuits in neuromorphic hardware, this lack of algorithmic insight also hinders technological progress in that area. The gold standard for learning in recurrent neural networks in machine learning is back-propagation through time (BPTT), which implements stochastic gradient descent with regard to a given loss function. But BPTT is unrealistic from a biological perspective, since it requires a transmission of error signals backwards in time and in space, i.e., from post- to presynaptic neurons. We show that an online merging of locally available information during a computation with suitable top-down learning signals in real-time provides highly capable approximations to BPTT. For tasks where information on errors arises only late during a network computation, we enrich locally available information through feedforward eligibility traces of synapses that can easily be computed in an online manner. The resulting new generation of learning algorithms for recurrent neural networks provides a new understanding of network learning in the brain that can be tested experimentally. In addition, these algorithms provide efficient methods for on-chip training of RSNNs in neuromorphic hardware.},
  language = {en},
  author = {Bellec, Guillaume and Scherr, Franz and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
  pages = {37},
  file = {/home/mo/Zotero/storage/9Q6ZYEPT/Bellec et al. - Biologically inspired alternatives to backpropagat.pdf}
}

@article{gruningSpikingNeuralNetworks2014,
  title = {Spiking {{Neural Networks}}: {{Principles}} and {{Challenges}}},
  abstract = {Over the last decade, various spiking neural network models have been proposed, along with a similarly increasing interest in spiking models of computation in computational neuroscience. The aim of this tutorial paper is to outline some of the common ground in state-of-the-art spiking neural networks as well as open challenges.},
  language = {en},
  journal = {Computational Intelligence},
  author = {Gruning, Andre and Bohte, Sander M},
  year = {2014},
  pages = {10},
  file = {/home/mo/Zotero/storage/VXNMHZDP/Gruning and Bohte - 2014 - Spiking Neural Networks Principles and Challenges.pdf}
}

@article{maassNetworksSpikingNeurons1997,
  title = {Networks of Spiking Neurons: {{The}} Third Generation of Neural Network Models},
  volume = {10},
  issn = {08936080},
  shorttitle = {Networks of Spiking Neurons},
  abstract = {The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on McCulloch Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In particular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, computationally more powerful than these other neural network models. A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that can be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology. \textcopyright{} 1997 Elsevier Science Ltd. All rights reserved.},
  language = {en},
  number = {9},
  journal = {Neural Networks},
  doi = {10.1016/S0893-6080(97)00011-7},
  author = {Maass, Wolfgang},
  month = dec,
  year = {1997},
  pages = {1659-1671},
  file = {/home/mo/Zotero/storage/649AXSKH/Maass - 1997 - Networks of spiking neurons The third generation .pdf}
}

@article{samadiDeepLearningDynamic2017,
  title = {Deep {{Learning}} with {{Dynamic Spiking Neurons}} and {{Fixed Feedback Weights}}},
  volume = {29},
  issn = {0899-7667, 1530-888X},
  language = {en},
  number = {3},
  journal = {Neural Computation},
  doi = {10.1162/NECO_a_00929},
  author = {Samadi, Arash and Lillicrap, Timothy P. and Tweed, Douglas B.},
  month = mar,
  year = {2017},
  pages = {578-602},
  file = {/home/mo/Zotero/storage/TSGPFGLM/Samadi et al. - 2017 - Deep Learning with Dynamic Spiking Neurons and Fix.pdf}
}

@article{noklandDirectFeedbackAlignment,
  title = {Direct {{Feedback Alignment Provides Learning}} in {{Deep Neural Networks}}},
  author = {N{\o}kland, Arild},
  file = {/home/mo/Zotero/storage/LLMS838S/1609.01596.pdf}
}

@misc{britzRecurrentNeuralNetworks2015,
  title = {Recurrent {{Neural Networks Tutorial}}, {{Part}} 1 \textendash{} {{Introduction}} to {{RNNs}}},
  abstract = {Recurrent Neural Networks (RNNs) are popular models that have shown great promise in many~NLP tasks. But despite their recent popularity I've only found a limited number of resources that thr\ldots{}},
  language = {en-US},
  journal = {WildML},
  author = {Britz, Denny},
  month = sep,
  year = {2015},
  file = {/home/mo/Zotero/storage/A3F4KS9V/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns.html}
}

@article{duanRLFastReinforcement2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.02779},
  primaryClass = {cs, stat},
  title = {{{RL}}\$\^2\$: {{Fast Reinforcement Learning}} via {{Slow Reinforcement Learning}}},
  shorttitle = {{{RL}}\$\^2\$},
  abstract = {Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a "fast" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL\$\^2\$, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose ("slow") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the "fast" RL algorithm on the current (previously unseen) MDP. We evaluate RL\$\^2\$ experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL\$\^2\$ is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL\$\^2\$ on a vision-based navigation task and show that it scales up to high-dimensional problems.},
  journal = {arXiv:1611.02779 [cs, stat]},
  author = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L. and Sutskever, Ilya and Abbeel, Pieter},
  month = nov,
  year = {2016},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/mo/Zotero/storage/I36RHRBL/Duan et al. - 2016 - RL$^2$ Fast Reinforcement Learning via Slow Reinf.pdf;/home/mo/Zotero/storage/A3CD5MTI/1611.html},
  annote = {Comment: 14 pages. Under review as a conference paper at ICLR 2017}
}

@article{maassIntroductionL2LLSTM,
  title = {Introduction to {{L2L}}, {{LSTM}} Networks, and Key Results from {{Wang}} et al., 2016},
  language = {en},
  author = {Maass, Wolfgang},
  pages = {12},
  file = {/home/mo/Zotero/storage/A6QXQ5YN/Maass - Introduction to L2L, LSTM networks, and key result.pdf}
}

@incollection{bellecLongShorttermMemory2018,
  title = {Long Short-Term Memory and {{Learning}}-to-Learn in Networks of Spiking Neurons},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  publisher = {{Curran Associates, Inc.}},
  author = {Bellec, Guillaume and Salaj, Darjan and Subramoney, Anand and Legenstein, Robert and Maass, Wolfgang},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {787--797},
  file = {/home/mo/Zotero/storage/YQQG36ZV/Bellec et al. - 2018 - Long short-term memory and Learning-to-learn in ne.pdf;/home/mo/Zotero/storage/GLTN3I55/7359-long-short-term-memory-and-learning-to-learn-in-networks-of-spiking-neurons.html}
}

@misc{2018L2LFuerberg,
  title = {2018 {{L2L Fuerberg}} - {{Anand}} - Document.Pdf},
  file = {/home/mo/Zotero/storage/JNEAEAZE/2018 L2L Fuerberg - Anand - document.pdf}
}

@article{liMetaSGDLearningLearn2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1707.09835},
  primaryClass = {cs},
  title = {Meta-{{SGD}}: {{Learning}} to {{Learn Quickly}} for {{Few}}-{{Shot Learning}}},
  shorttitle = {Meta-{{SGD}}},
  abstract = {Few-shot learning is challenging for learning algorithms that learn each task in isolation and from scratch. In contrast, meta-learning learns from many related tasks a meta-learner that can learn a new task more accurately and faster with fewer examples, where the choice of meta-learners is crucial. In this paper, we develop Meta-SGD, an SGD-like, easily trainable meta-learner that can initialize and adapt any differentiable learner in just one step, on both supervised learning and reinforcement learning. Compared to the popular meta-learner LSTM, Meta-SGD is conceptually simpler, easier to implement, and can be learned more efficiently. Compared to the latest meta-learner MAML, Meta-SGD has a much higher capacity by learning to learn not just the learner initialization, but also the learner update direction and learning rate, all in a single meta-learning process. Meta-SGD shows highly competitive performance for few-shot learning on regression, classification, and reinforcement learning.},
  language = {en},
  journal = {arXiv:1707.09835 [cs]},
  author = {Li, Zhenguo and Zhou, Fengwei and Chen, Fei and Li, Hang},
  month = jul,
  year = {2017},
  keywords = {Computer Science - Machine Learning},
  file = {/home/mo/Zotero/storage/PB8B3T72/Li et al. - 2017 - Meta-SGD Learning to Learn Quickly for Few-Shot L.pdf},
  annote = {Comment: reinforcement learning included, 20-way classification on MiniImagenet included}
}

@article{tangGridbotAutonomousRobot2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1807.02155},
  title = {Gridbot: {{An}} Autonomous Robot Controlled by a {{Spiking Neural Network}} Mimicking the Brain's Navigational System},
  shorttitle = {Gridbot},
  abstract = {It is true that the "best" neural network is not necessarily the one with the most "brain-like" behavior. Understanding biological intelligence, however, is a fundamental goal for several distinct disciplines. Translating our understanding of intelligence to machines is a fundamental problem in robotics. Propelled by new advancements in Neuroscience, we developed a spiking neural network (SNN) that draws from mounting experimental evidence that a number of individual neurons is associated with spatial navigation. By following the brain's structure, our model assumes no initial all-to-all connectivity, which could inhibit its translation to a neuromorphic hardware, and learns an uncharted territory by mapping its identified components into a limited number of neural representations, through spike-timing dependent plasticity (STDP). In our ongoing effort to employ a bioinspired SNN-controlled robot to real-world spatial mapping applications, we demonstrate here how an SNN may robustly control an autonomous robot in mapping and exploring an unknown environment, while compensating for its own intrinsic hardware imperfections, such as partial or total loss of visual input.},
  journal = {Proceedings of the International Conference on Neuromorphic Systems  - ICONS '18},
  doi = {10.1145/3229884.3229888},
  author = {Tang, Guangzhi and Michmizos, Konstantinos P.},
  year = {2018},
  keywords = {Computer Science - Artificial Intelligence,Quantitative Biology - Neurons and Cognition},
  pages = {1-8},
  file = {/home/mo/Zotero/storage/BTB8HYRE/Tang and Michmizos - 2018 - Gridbot An autonomous robot controlled by a Spiki.pdf;/home/mo/Zotero/storage/UEW7PF2C/1807.html},
  annote = {Comment: 8 pages, 3 Figures, International Conference on Neuromorphic Systems (ICONS 2018)}
}

@article{botvinickReinforcementLearningFast2019,
  title = {Reinforcement {{Learning}}, {{Fast}} and {{Slow}}},
  volume = {23},
  issn = {1879-307X},
  abstract = {Deep reinforcement learning (RL) methods have driven impressive advances in artificial intelligence in recent years, exceeding human performance in domains ranging from Atari to Go to no-limit poker. This progress has drawn the attention of cognitive scientists interested in understanding human learning. However, the concern has been raised that deep RL may be too sample-inefficient - that is, it may simply be too slow - to provide a plausible model of how humans learn. In the present review, we counter this critique by describing recently developed techniques that allow deep RL to operate more nimbly, solving problems much more quickly than previous methods. Although these techniques were developed in an AI context, we propose that they may have rich implications for psychology and neuroscience. A key insight, arising from these AI methods, concerns the fundamental connection between fast RL and slower, more incremental forms of learning.},
  language = {eng},
  number = {5},
  journal = {Trends in Cognitive Sciences},
  doi = {10.1016/j.tics.2019.02.006},
  author = {Botvinick, Matthew and Ritter, Sam and Wang, Jane X. and {Kurth-Nelson}, Zeb and Blundell, Charles and Hassabis, Demis},
  month = may,
  year = {2019},
  pages = {408-422},
  file = {/home/mo/Zotero/storage/FB8RG8W8/Botvinick et al. - 2019 - Reinforcement Learning, Fast and Slow.pdf},
  pmid = {31003893}
}

@article{lansdellLearningtolearn2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1811.00231},
  primaryClass = {cs, q-bio},
  title = {Towards Learning-to-Learn},
  abstract = {In good old-fashioned artificial intelligence (GOFAI), humans specified systems that solved problems. Much of the recent progress in AI has come from replacing human insights by learning. However, learning itself is still usually built by humans -- specifically the choice that parameter updates should follow the gradient of a cost function. Yet, in analogy with GOFAI, there is no reason to believe that humans are particularly good at defining such learning systems: we may expect learning itself to be better if we learn it. Recent research in machine learning has started to realize the benefits of that strategy. We should thus expect this to be relevant for neuroscience: how could the correct learning rules be acquired? Indeed, cognitive science has long shown that humans learn-to-learn, which is potentially responsible for their impressive learning abilities. Here we discuss ideas across machine learning, neuroscience, and cognitive science that matter for the principle of learning-to-learn.},
  journal = {arXiv:1811.00231 [cs, q-bio]},
  author = {Lansdell, Benjamin James and Kording, Konrad Paul},
  month = nov,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition},
  file = {/home/mo/Zotero/storage/7VXBDN8Y/Lansdell and Kording - 2018 - Towards learning-to-learn.pdf;/home/mo/Zotero/storage/SUI8TEA2/1811.html},
  annote = {Comment: 8 pages, 1 figure}
}

@article{wulfmeierMutualAlignmentTransfer2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1707.07907},
  primaryClass = {cs},
  title = {Mutual {{Alignment Transfer Learning}}},
  abstract = {Training robots for operation in the real world is a complex, time consuming and potentially expensive task. Despite significant success of reinforcement learning in games and simulations, research in real robot applications has not been able to match similar progress. While sample complexity can be reduced by training policies in simulation, such policies can perform sub-optimally on the real platform given imperfect calibration of model dynamics. We present an approach -- supplemental to fine tuning on the real robot -- to further benefit from parallel access to a simulator during training and reduce sample requirements on the real robot. The developed approach harnesses auxiliary rewards to guide the exploration for the real world agent based on the proficiency of the agent in simulation and vice versa. In this context, we demonstrate empirically that the reciprocal alignment for both agents provides further benefit as the agent in simulation can adjust to optimize its behaviour for states commonly visited by the real-world agent.},
  journal = {arXiv:1707.07907 [cs]},
  author = {Wulfmeier, Markus and Posner, Ingmar and Abbeel, Pieter},
  month = jul,
  year = {2017},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/mo/Zotero/storage/U8FXB6KB/Wulfmeier et al. - 2017 - Mutual Alignment Transfer Learning.pdf;/home/mo/Zotero/storage/NBE7CGH3/1707.html}
}

@article{chenLearningLearnGradient2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.03824},
  primaryClass = {cs, stat},
  title = {Learning to {{Learn}} without {{Gradient Descent}} by {{Gradient Descent}}},
  abstract = {We learn recurrent neural network optimizers trained on simple synthetic functions by gradient descent. We show that these learned optimizers exhibit a remarkable degree of transfer in that they can be used to efficiently optimize a broad range of derivative-free black-box functions, including Gaussian process bandits, simple control objectives, global optimization benchmarks and hyper-parameter tuning tasks. Up to the training horizon, the learned optimizers learn to trade-off exploration and exploitation, and compare favourably with heavily engineered Bayesian optimization packages for hyper-parameter tuning.},
  journal = {arXiv:1611.03824 [cs, stat]},
  author = {Chen, Yutian and Hoffman, Matthew W. and Colmenarejo, Sergio Gomez and Denil, Misha and Lillicrap, Timothy P. and Botvinick, Matt and {de Freitas}, Nando},
  month = nov,
  year = {2016},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/mo/Zotero/storage/FNCAVGHP/Chen et al. - 2016 - Learning to Learn without Gradient Descent by Grad.pdf;/home/mo/Zotero/storage/58TCELX7/1611.html},
  annote = {Comment: Accepted by ICML 2017. Previous version "Learning to Learn for Global Optimization of Black Box Functions" was published in the Deep Reinforcement Learning Workshop, NIPS 2016}
}

@article{kaiserEmbodiedNeuromorphicVision2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1904.04805},
  primaryClass = {cs},
  title = {Embodied {{Neuromorphic Vision}} with {{Event}}-{{Driven Random Backpropagation}}},
  abstract = {Spike-based communication between biological neurons is sparse and unreliable. This enables the brain to process visual information from the eyes efficiently. Taking inspiration from biology, artificial spiking neural networks coupled with silicon retinas attempt to model these computations. Recent findings in machine learning allowed the derivation of a family of powerful synaptic plasticity rules approximating backpropagation for spiking networks. Are these rules capable of processing real-world visual sensory data? In this paper, we evaluate the performance of Event-Driven Random Back-Propagation (eRBP) at learning representations from event streams provided by a Dynamic Vision Sensor (DVS). First, we show that eRBP matches state-of-the-art performance on the DvsGesture dataset with the addition of a simple covert attention mechanism. By remapping visual receptive fields relatively to the center of the motion, this attention mechanism provides translation invariance at low computational cost compared to convolutions. Second, we successfully integrate eRBP in a real robotic setup, where a robotic arm grasps objects according to detected visual affordances. In this setup, visual information is actively sensed by a DVS mounted on a robotic head performing microsaccadic eye movements. We show that our method classifies affordances within 100ms after microsaccade onset, which is comparable to human performance reported in behavioral study. Our results suggest that advances in neuromorphic technology and plasticity rules enable the development of autonomous robots operating at high speed and low energy consumption.},
  journal = {arXiv:1904.04805 [cs]},
  author = {Kaiser, Jacques and Friedrich, Alexander and Tieck, J. Camilo Vasquez and Reichard, Daniel and Roennau, Arne and Neftci, Emre and Dillmann, R{\"u}diger},
  month = apr,
  year = {2019},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/mo/Zotero/storage/YHZHHSE2/Kaiser et al. - 2019 - Embodied Neuromorphic Vision with Event-Driven Ran.pdf;/home/mo/Zotero/storage/MSRNKRPJ/1904.html},
  annote = {Comment: v2: title update, better plots and wordings. 8 pages, 9 figures, 1 table, video: https://neurorobotics-files.net/index.php/s/sBQzWFrBPoH9Dx7}
}

@article{ponghiranReinforcementLearningLowComplexity2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1906.01695},
  primaryClass = {cs, stat},
  title = {Reinforcement {{Learning}} with {{Low}}-{{Complexity Liquid State Machines}}},
  abstract = {We propose reinforcement learning on simple networks consisting of random connections of spiking neurons (both recurrent and feed-forward) that can learn complex tasks with very little trainable parameters. Such sparse and randomly interconnected recurrent spiking networks exhibit highly non-linear dynamics that transform the inputs into rich high-dimensional representations based on past context. The random input representations can be efficiently interpreted by an output (or readout) layer with trainable parameters. Systematic initialization of the random connections and training of the readout layer using Q-learning algorithm enable such small random spiking networks to learn optimally and achieve the same learning efficiency as humans on complex reinforcement learning tasks like Atari games. The spike-based approach using small random recurrent networks provides a computationally efficient alternative to state-of-the-art deep reinforcement learning networks with several layers of trainable parameters. The low-complexity spiking networks can lead to improved energy efficiency in event-driven neuromorphic hardware for complex reinforcement learning tasks.},
  journal = {arXiv:1906.01695 [cs, stat]},
  author = {Ponghiran, Wachirawit and Srinivasan, Gopalakrishnan and Roy, Kaushik},
  month = jun,
  year = {2019},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/mo/Zotero/storage/V6FY2QAN/Ponghiran et al. - 2019 - Reinforcement Learning with Low-Complexity Liquid .pdf;/home/mo/Zotero/storage/ED72IZ3V/1906.html},
  annote = {Comment: 6 figures}
}

@article{schweighoferMetalearningReinforcementLearning2003,
  title = {Meta-Learning in {{Reinforcement Learning}}},
  volume = {16},
  issn = {0893-6080},
  abstract = {Meta-parameters in reinforcement learning should be tuned to the environmental dynamics and the animal performance. Here, we propose a biologically plausible meta-reinforcement learning algorithm for tuning these meta-parameters in a dynamic, adaptive manner. We tested our algorithm in both a simulation of a Markov decision task and in a non-linear control task. Our results show that the algorithm robustly finds appropriate meta-parameter values, and controls the meta-parameter time course, in both static and dynamic environments. We suggest that the phasic and tonic components of dopamine neuron firing can encode the signal required for meta-learning of reinforcement learning.},
  number = {1},
  journal = {Neural Networks},
  doi = {10.1016/S0893-6080(02)00228-9},
  author = {Schweighofer, Nicolas and Doya, Kenji},
  month = jan,
  year = {2003},
  keywords = {Dopamine,Dynamic environment,Meta-learning,Meta-parameters,Neuromodulation,Reinforcement learning,TD error},
  pages = {5-9},
  file = {/home/mo/Zotero/storage/8CHLX463/S0893608002002289.html}
}

@article{pfeifferDeepLearningSpiking2018,
  title = {Deep {{Learning With Spiking Neurons}}: {{Opportunities}} and {{Challenges}}},
  volume = {12},
  issn = {1662-4548},
  shorttitle = {Deep {{Learning With Spiking Neurons}}},
  abstract = {Spiking neural networks (SNNs) are inspired by information processing in biology, where sparse and asynchronous binary signals are communicated and processed in a massively parallel fashion. SNNs on neuromorphic hardware exhibit favorable properties such as low power consumption, fast inference, and event-driven information processing. This makes them interesting candidates for the efficient implementation of deep neural networks, the method of choice for many machine learning tasks. In this review, we address the opportunities that deep spiking networks offer and investigate in detail the challenges associated with training SNNs in a way that makes them competitive with conventional deep learning, but simultaneously allows for efficient mapping to hardware. A wide range of training methods for SNNs is presented, ranging from the conversion of conventional deep networks into SNNs, constrained training before conversion, spiking variants of backpropagation, and biologically motivated variants of STDP. The goal of our review is to define a categorization of SNN training methods, and summarize their advantages and drawbacks. We further discuss relationships between SNNs and binary networks, which are becoming popular for efficient digital hardware implementation. Neuromorphic hardware platforms have great potential to enable deep spiking networks in real-world applications. We compare the suitability of various neuromorphic systems that have been developed over the past years, and investigate potential use cases. Neuromorphic approaches and conventional machine learning should not be considered simply two solutions to the same classes of problems, instead it is possible to identify and exploit their task-specific advantages. Deep SNNs offer great opportunities to work with new types of event-based sensors, exploit temporal codes and local on-chip learning, and we have so far just scratched the surface of realizing these advantages in practical applications.},
  journal = {Frontiers in Neuroscience},
  doi = {10.3389/fnins.2018.00774},
  author = {Pfeiffer, Michael and Pfeil, Thomas},
  month = oct,
  year = {2018},
  file = {/home/mo/Zotero/storage/SI97ECUL/Pfeiffer and Pfeil - 2018 - Deep Learning With Spiking Neurons Opportunities .pdf},
  pmid = {30410432},
  pmcid = {PMC6209684}
}

@article{tavanaeiDeepLearningSpiking2019,
  title = {Deep Learning in Spiking Neural Networks},
  volume = {111},
  issn = {0893-6080},
  abstract = {In recent years, deep learning has revolutionized the field of machine learning, for computer vision in particular. In this approach, a deep (multilayer) artificial neural network (ANN) is trained, most often in a supervised manner using backpropagation. Vast amounts of labeled training examples are required, but the resulting classification accuracy is truly impressive, sometimes outperforming humans. Neurons in an ANN are characterized by a single, static, continuous-valued activation. Yet biological neurons use discrete spikes to compute and transmit information, and the spike times, in addition to the spike rates, matter. Spiking neural networks (SNNs) are thus more biologically realistic than ANNs, and are arguably the only viable option if one wants to understand how the brain computes at the neuronal description level. The spikes of biological neurons are sparse in time and space, and event-driven. Combined with bio-plausible local learning rules, this makes it easier to build low-power, neuromorphic hardware for SNNs. However, training deep SNNs remains a challenge. Spiking neurons' transfer function is usually non-differentiable, which prevents using backpropagation. Here we review recent supervised and unsupervised methods to train deep SNNs, and compare them in terms of accuracy and computational cost. The emerging picture is that SNNs still lag behind ANNs in terms of accuracy, but the gap is decreasing, and can even vanish on some tasks, while SNNs typically require many fewer operations and are the better candidates to process spatio-temporal data.},
  journal = {Neural Networks},
  doi = {10.1016/j.neunet.2018.12.002},
  author = {Tavanaei, Amirhossein and Ghodrati, Masoud and Kheradpisheh, Saeed Reza and Masquelier, Timoth{\'e}e and Maida, Anthony},
  month = mar,
  year = {2019},
  keywords = {Biological plausibility,Deep learning,Machine learning,Power-efficient architecture,Spiking neural network},
  pages = {47-63},
  file = {/home/mo/Zotero/storage/BWQXVANU/Tavanaei et al. - 2019 - Deep learning in spiking neural networks.pdf;/home/mo/Zotero/storage/C6VK3R2J/S0893608018303332.html}
}

@incollection{florianReinforcementLearningSpiking2012,
  address = {{Boston, MA}},
  title = {Reinforcement {{Learning}} in {{Spiking Neural Networks}}},
  isbn = {978-1-4419-1427-9 978-1-4419-1428-6},
  language = {en},
  booktitle = {Encyclopedia of the {{Sciences}} of {{Learning}}},
  publisher = {{Springer US}},
  author = {Florian, R{\u a}zvan V.},
  editor = {Seel, Norbert M.},
  year = {2012},
  pages = {2802-2803},
  doi = {10.1007/978-1-4419-1428-6_1713}
}

@article{evansReinforcementLearningNeurally2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.06096},
  primaryClass = {cs},
  title = {Reinforcement {{Learning}} in a {{Neurally Controlled Robot Using Dopamine Modulated STDP}}},
  abstract = {Recent work has shown that dopamine-modulated STDP can solve many of the issues associated with reinforcement learning, such as the distal reward problem. Spiking neural networks provide a useful technique in implementing reinforcement learning in an embodied context as they can deal with continuous parameter spaces and as such are better at generalizing the correct behaviour to perform in a given context. In this project we implement a version of DA-modulated STDP in an embodied robot on a food foraging task. Through simulated dopaminergic neurons we show how the robot is able to learn a sequence of behaviours in order to achieve a food reward. In tests the robot was able to learn food-attraction behaviour, and subsequently unlearn this behaviour when the environment changed, in all 50 trials. Moreover we show that the robot is able to operate in an environment whereby the optimal behaviour changes rapidly and so the agent must constantly relearn. In a more complex environment, consisting of food-containers, the robot was able to learn food-container attraction in 95\% of trials, despite the large temporal distance between the correct behaviour and the reward. This is achieved by shifting the dopamine response from the primary stimulus (food) to the secondary stimulus (food-container). Our work provides insights into the reasons behind some observed biological phenomena, such as the bursting behaviour observed in dopaminergic neurons. As well as demonstrating how spiking neural network controlled robots are able to solve a range of reinforcement learning tasks.},
  journal = {arXiv:1502.06096 [cs]},
  author = {Evans, Richard},
  month = feb,
  year = {2015},
  keywords = {Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics},
  file = {/home/mo/Zotero/storage/I5AEF644/Evans - 2015 - Reinforcement Learning in a Neurally Controlled Ro.pdf;/home/mo/Zotero/storage/KNV6BT54/1502.html}
}

@inproceedings{florianReinforcementLearningAlgorithm2005a,
  address = {{Timisoara, Romania}},
  title = {A Reinforcement Learning Algorithm for Spiking Neural Networks},
  isbn = {978-0-7695-2453-5},
  abstract = {The paper presents a new reinforcement learning mechanism for spiking neural networks. The algorithm is derived for networks of stochastic integrate-and-fire neurons, but it can be also applied to generic spiking neural networks. Learning is achieved by synaptic changes that depend on the firing of pre- and postsynaptic neurons, and that are modulated with a global reinforcement signal. The efficacy of the algorithm is verified in a biologically-inspired experiment, featuring a simulated worm that searches for food. Our model recovers a form of neural plasticity experimentally observed in animals, combining spike-timing-dependent synaptic changes of one sign with non-associative synaptic changes of the opposite sign determined by presynaptic spikes. The model also predicts that the time constant of spike-timing-dependent synaptic changes is equal to the membrane time constant of the neuron, in agreement with experimental observations in the brain. This study also led to the discovery of a biologically-plausible reinforcement learning mechanism that works by modulating spike-timing-dependent plasticity (STDP) with a global reward signal.},
  language = {en},
  booktitle = {Seventh {{International Symposium}} on {{Symbolic}} and {{Numeric Algorithms}} for {{Scientific Computing}} ({{SYNASC}}'05)},
  publisher = {{IEEE}},
  doi = {10.1109/SYNASC.2005.13},
  author = {Florian, R.V.},
  year = {2005},
  pages = {8 pp.},
  file = {/home/mo/Zotero/storage/9RLYP5QQ/Florian - 2005 - A reinforcement learning algorithm for spiking neu.pdf}
}

@article{florianReinforcementLearningModulation2007,
  title = {Reinforcement {{Learning Through Modulation}} of {{Spike}}-{{Timing}}-{{Dependent Synaptic Plasticity}}},
  volume = {19},
  issn = {0899-7667},
  abstract = {The persistent modification of synaptic efficacy as a function of the relative timing of pre- and postsynaptic spikes is a phenomenon known as spike-timing-dependent plasticity (STDP). Here we show that the modulation of STDP by a global reward signal leads to reinforcement learning. We first derive analytically learning rules involving reward-modulated spike-timing-dependent synaptic and intrinsic plasticity, by applying a reinforcement learning algorithm to the stochastic spike response model of spiking neurons. These rules have several features common to plasticity mechanisms experimentally found in the brain. We then demonstrate in simulations of networks of integrate-and-fire neurons the efficacy of two simple learning rules involving modulated STDP. One rule is a direct extension of the standard STDP model (modulated STDP), and the other one involves an eligibility trace stored at each synapse that keeps a decaying memory of the relationships between the recent pairs of pre- and postsynaptic spike pairs (modulated STDP with eligibility trace). This latter rule permits learning even if the reward signal is delayed. The proposed rules are able to solve the XOR problem with both rate coded and temporally coded input and to learn a target output firing-rate pattern. These learning rules are biologically plausible, may be used for training generic artificial spiking neural networks, regardless of the neural model used, and suggest the experimental investigation in animals of the existence of reward-modulated STDP.},
  number = {6},
  journal = {Neural Computation},
  doi = {10.1162/neco.2007.19.6.1468},
  author = {Florian, R{\u a}zvan V.},
  month = apr,
  year = {2007},
  pages = {1468-1502},
  file = {/home/mo/Zotero/storage/FJN7AWIT/neco.2007.19.6.html}
}

@article{seungLearningSpikingNeural2003,
  title = {Learning in {{Spiking Neural Networks}} by {{Reinforcement}} of {{Stochastic Synaptic Transmission}}},
  volume = {40},
  issn = {0896-6273},
  abstract = {It is well-known that chemical synaptic transmission is an unreliable process, but the function of such unreliability remains unclear. Here I consider the hypothesis that the randomness of synaptic transmission is harnessed by the brain for learning, in analogy to the way that genetic mutation is utilized by Darwinian evolution. This is possible if synapses are ``hedonistic,'' responding to a global reward signal by increasing their probabilities of vesicle release or failure, depending on which action immediately preceded reward. Hedonistic synapses learn by computing a stochastic approximation to the gradient of the average reward. They are compatible with synaptic dynamics such as short-term facilitation and depression and with the intricacies of dendritic integration and action potential generation. A network of hedonistic synapses can be trained to perform a desired computation by administering reward appropriately, as illustrated here through numerical simulations of integrate-and-fire model neurons.},
  number = {6},
  journal = {Neuron},
  doi = {10.1016/S0896-6273(03)00761-X},
  author = {Seung, H. Sebastian},
  month = dec,
  year = {2003},
  pages = {1063-1073},
  file = {/home/mo/Zotero/storage/UYKCTSB3/Seung - 2003 - Learning in Spiking Neural Networks by Reinforceme.pdf;/home/mo/Zotero/storage/M2GEUNVQ/S089662730300761X.html}
}

@article{sboevSpikingNeuralNetwork2018,
  series = {Postproceedings of the 9th {{Annual International Conference}} on {{Biologically Inspired Cognitive Architectures}}, {{BICA}} 2018 ({{Ninth Annual Meeting}} of the {{BICA Society}}), Held {{August}} 22-24, 2018 in {{Prague}}, {{Czech Republic}}},
  title = {Spiking Neural Network Reinforcement Learning Method Based on Temporal Coding and {{STDP}}},
  volume = {145},
  issn = {1877-0509},
  abstract = {A method to train a spiking network to solve a classification task using Spike-Timing-Dependent Plasticity is proposed. Learning is based on the ability of STDP to memorize repeating spike patterns: earliest spikes of pattern contribute to output spike and those weights stay high, while others weights falls down to zero. The output neurons are provided with information on the classes by stimulating the neuron corresponding to the desired class to fire an early spike. The network is single-layer, with competition introduced by inhibitory interconnections. The network consists of leaky integrate-and-fire neurons tuned to provide one spike per pattern. Input data is encoded by Gaussian receptive fields, where earliest spikes contains the most information. The learning method is tested on Fisher's Iris and Wisconsin Breast Cancer datasets, and results compared to Support Vector Machines, Random Forest and formal neural networks with Adam optimizer.},
  journal = {Procedia Computer Science},
  doi = {10.1016/j.procs.2018.11.107},
  author = {Sboev, Alexander and Vlasov, Danila and Rybka, Roman and Serenko, Alexey},
  month = jan,
  year = {2018},
  keywords = {classification,reinforcement learning,spike-timing-dependent plasticity,spiking neural network,temporal coding},
  pages = {458-463},
  file = {/home/mo/Zotero/storage/BQI9SBRE/Sboev et al. - 2018 - Spiking neural network reinforcement learning meth.pdf;/home/mo/Zotero/storage/GKL4GYLQ/S1877050918323950.html}
}


