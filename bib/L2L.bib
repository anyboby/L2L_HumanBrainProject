
@article{wangLearningReinforcementLearn2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.05763},
  primaryClass = {cs, stat},
  title = {Learning to Reinforcement Learn},
  abstract = {In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience.},
  language = {en},
  journal = {arXiv:1611.05763 [cs, stat]},
  author = {Wang, Jane X. and {Kurth-Nelson}, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z. and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},
  month = nov,
  year = {2016},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/mo/Zotero/storage/V9KHIKLI/Wang et al. - 2016 - Learning to reinforcement learn.pdf}
}

@article{harbInvestigatingRecurrenceEligibility,
  title = {Investigating {{Recurrence}} and {{Eligibility Traces}} in {{Deep Q}}-{{Networks}}},
  abstract = {Eligibility traces in reinforcement learning are used as a bias-variance trade-off and can often speed up training time by propagating knowledge back over time-steps in a single update. We investigate the use of eligibility traces in combination with recurrent networks in the Atari domain. We illustrate the benefits of both recurrent nets and eligibility traces in some Atari games, and highlight also the importance of the optimization used in the training.},
  language = {en},
  author = {Harb, Jean and Precup, Doina},
  pages = {8},
  file = {/home/mo/Zotero/storage/HC55894S/Harb and Precup - Investigating Recurrence and Eligibility Traces in.pdf}
}

@article{bellecBiologicallyInspiredAlternatives,
  title = {Biologically Inspired Alternatives to Backpropagation through Time for Learning in Recurrent Neural Nets},
  abstract = {The way how recurrently connected networks of spiking neurons in the brain acquire powerful information processing capabilities through learning has remained a mystery. This lack of understanding is linked to a lack of learning algorithms for recurrent networks of spiking neurons (RSNNs) that are both functionally powerful and can be implemented by known biological mechanisms. Since RSNNs are simultaneously a primary target for implementations of brain-inspired circuits in neuromorphic hardware, this lack of algorithmic insight also hinders technological progress in that area. The gold standard for learning in recurrent neural networks in machine learning is back-propagation through time (BPTT), which implements stochastic gradient descent with regard to a given loss function. But BPTT is unrealistic from a biological perspective, since it requires a transmission of error signals backwards in time and in space, i.e., from post- to presynaptic neurons. We show that an online merging of locally available information during a computation with suitable top-down learning signals in real-time provides highly capable approximations to BPTT. For tasks where information on errors arises only late during a network computation, we enrich locally available information through feedforward eligibility traces of synapses that can easily be computed in an online manner. The resulting new generation of learning algorithms for recurrent neural networks provides a new understanding of network learning in the brain that can be tested experimentally. In addition, these algorithms provide efficient methods for on-chip training of RSNNs in neuromorphic hardware.},
  language = {en},
  author = {Bellec, Guillaume and Scherr, Franz and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
  pages = {37},
  file = {/home/mo/Zotero/storage/9Q6ZYEPT/Bellec et al. - Biologically inspired alternatives to backpropagat.pdf}
}

@article{gruningSpikingNeuralNetworks2014,
  title = {Spiking {{Neural Networks}}: {{Principles}} and {{Challenges}}},
  abstract = {Over the last decade, various spiking neural network models have been proposed, along with a similarly increasing interest in spiking models of computation in computational neuroscience. The aim of this tutorial paper is to outline some of the common ground in state-of-the-art spiking neural networks as well as open challenges.},
  language = {en},
  journal = {Computational Intelligence},
  author = {Gruning, Andre and Bohte, Sander M},
  year = {2014},
  pages = {10},
  file = {/home/mo/Zotero/storage/VXNMHZDP/Gruning and Bohte - 2014 - Spiking Neural Networks Principles and Challenges.pdf}
}

@article{maassNetworksSpikingNeurons1997,
  title = {Networks of Spiking Neurons: {{The}} Third Generation of Neural Network Models},
  volume = {10},
  issn = {08936080},
  shorttitle = {Networks of Spiking Neurons},
  abstract = {The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on McCulloch Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In particular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, computationally more powerful than these other neural network models. A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that can be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology. \textcopyright{} 1997 Elsevier Science Ltd. All rights reserved.},
  language = {en},
  number = {9},
  journal = {Neural Networks},
  doi = {10.1016/S0893-6080(97)00011-7},
  author = {Maass, Wolfgang},
  month = dec,
  year = {1997},
  pages = {1659-1671},
  file = {/home/mo/Zotero/storage/649AXSKH/Maass - 1997 - Networks of spiking neurons The third generation .pdf}
}

@article{samadiDeepLearningDynamic2017,
  title = {Deep {{Learning}} with {{Dynamic Spiking Neurons}} and {{Fixed Feedback Weights}}},
  volume = {29},
  issn = {0899-7667, 1530-888X},
  language = {en},
  number = {3},
  journal = {Neural Computation},
  doi = {10.1162/NECO_a_00929},
  author = {Samadi, Arash and Lillicrap, Timothy P. and Tweed, Douglas B.},
  month = mar,
  year = {2017},
  pages = {578-602},
  file = {/home/mo/Zotero/storage/TSGPFGLM/Samadi et al. - 2017 - Deep Learning with Dynamic Spiking Neurons and Fix.pdf}
}

@article{noklandDirectFeedbackAlignment,
  title = {Direct {{Feedback Alignment Provides Learning}} in {{Deep Neural Networks}}},
  author = {N{\o}kland, Arild},
  file = {/home/mo/Zotero/storage/LLMS838S/1609.01596.pdf}
}

@misc{britzRecurrentNeuralNetworks2015,
  title = {Recurrent {{Neural Networks Tutorial}}, {{Part}} 1 \textendash{} {{Introduction}} to {{RNNs}}},
  abstract = {Recurrent Neural Networks (RNNs) are popular models that have shown great promise in many~NLP tasks. But despite their recent popularity I've only found a limited number of resources that thr\ldots{}},
  language = {en-US},
  journal = {WildML},
  author = {Britz, Denny},
  month = sep,
  year = {2015},
  file = {/home/mo/Zotero/storage/A3F4KS9V/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns.html}
}

@article{duanRLFastReinforcement2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.02779},
  primaryClass = {cs, stat},
  title = {{{RL}}\$\^2\$: {{Fast Reinforcement Learning}} via {{Slow Reinforcement Learning}}},
  shorttitle = {{{RL}}\$\^2\$},
  abstract = {Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a "fast" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL\$\^2\$, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose ("slow") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the "fast" RL algorithm on the current (previously unseen) MDP. We evaluate RL\$\^2\$ experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL\$\^2\$ is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL\$\^2\$ on a vision-based navigation task and show that it scales up to high-dimensional problems.},
  journal = {arXiv:1611.02779 [cs, stat]},
  author = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L. and Sutskever, Ilya and Abbeel, Pieter},
  month = nov,
  year = {2016},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/mo/Zotero/storage/I36RHRBL/Duan et al. - 2016 - RL$^2$ Fast Reinforcement Learning via Slow Reinf.pdf;/home/mo/Zotero/storage/A3CD5MTI/1611.html},
  annote = {Comment: 14 pages. Under review as a conference paper at ICLR 2017}
}

@article{jangReinforcementCarRacing,
  title = {Reinforcement {{Car Racing}} with {{A3C}}},
  abstract = {In this paper, we introduce a modification of asynchronous advantage actor-critic algorithm, namely ``A3C with continuous certainty.'' We show that the new approach is particularly powerful in solving complex environment with continuous action space such as OpenAI Gym CarRacing-v0 environment. While there have been remarkable breakthroughs in combining CNNs and deep reinforcement learning architecture together to solve simple games such as Atari games(Volodymyr Mnih, 2013)(Volodymyr Mnih, 2015), car racing games, including this particular OpenAI Gym environment, contain many unique challenges of high-dimensional input state, continuous multidimensional action space, and rapidly changing environment. In addition, prior models including double deep Q network(Hado van Hasselt, 2016) and deep deterministic policy gradient(Timothy P. Lillicrap, 2015) do not give a good performance when it comes to an environment with continuous action space. In Section 4, we show that A3C with continuous certainty gives us competitive performance and how it was able to rank the fourth on OpenAI Gym leaderboard. The modification boosted the performance by about 100.0 from the vanilla A3C. We explore architectural variations such as action space control, number of threads for A3C, and CNN layer depths.},
  language = {en},
  author = {Jang, Se Won and Min, Jesik and Kim, Jae Hyun},
  pages = {8},
  file = {/home/mo/Zotero/storage/EHZS9NJ6/Jang et al. - Reinforcement Car Racing with A3C.pdf}
}

@article{maassIntroductionL2LLSTM,
  title = {Introduction to {{L2L}}, {{LSTM}} Networks, and Key Results from {{Wang}} et al., 2016},
  language = {en},
  author = {Maass, Wolfgang},
  pages = {12},
  file = {/home/mo/Zotero/storage/A6QXQ5YN/Maass - Introduction to L2L, LSTM networks, and key result.pdf}
}

@incollection{bellecLongShorttermMemory2018,
  title = {Long Short-Term Memory and {{Learning}}-to-Learn in Networks of Spiking Neurons},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  publisher = {{Curran Associates, Inc.}},
  author = {Bellec, Guillaume and Salaj, Darjan and Subramoney, Anand and Legenstein, Robert and Maass, Wolfgang},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {787--797},
  file = {/home/mo/Zotero/storage/YQQG36ZV/Bellec et al. - 2018 - Long short-term memory and Learning-to-learn in ne.pdf;/home/mo/Zotero/storage/GLTN3I55/7359-long-short-term-memory-and-learning-to-learn-in-networks-of-spiking-neurons.html}
}

@misc{2018L2LFuerberg,
  title = {2018 {{L2L Fuerberg}} - {{Anand}} - Document.Pdf},
  file = {/home/mo/Zotero/storage/JNEAEAZE/2018 L2L Fuerberg - Anand - document.pdf}
}

@article{lillicrapContinuousControlDeep2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1509.02971},
  primaryClass = {cs, stat},
  title = {Continuous Control with Deep Reinforcement Learning},
  abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
  journal = {arXiv:1509.02971 [cs, stat]},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  month = sep,
  year = {2015},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/mo/Zotero/storage/D5C8ZWP8/Lillicrap et al. - 2015 - Continuous control with deep reinforcement learnin.pdf;/home/mo/Zotero/storage/GPMIAA9A/1509.html},
  annote = {Comment: 10 pages + supplementary}
}

@article{dosovitskiyCARLAOpenUrban2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.03938},
  primaryClass = {cs},
  title = {{{CARLA}}: {{An Open Urban Driving Simulator}}},
  shorttitle = {{{CARLA}}},
  abstract = {We introduce CARLA, an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites and environmental conditions. We use CARLA to study the performance of three approaches to autonomous driving: a classic modular pipeline, an end-to-end model trained via imitation learning, and an end-to-end model trained via reinforcement learning. The approaches are evaluated in controlled scenarios of increasing difficulty, and their performance is examined via metrics provided by CARLA, illustrating the platform's utility for autonomous driving research. The supplementary video can be viewed at https://youtu.be/Hp8Dz-Zek2E},
  journal = {arXiv:1711.03938 [cs]},
  author = {Dosovitskiy, Alexey and Ros, German and Codevilla, Felipe and Lopez, Antonio and Koltun, Vladlen},
  month = nov,
  year = {2017},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {/home/mo/Zotero/storage/WRHBBIX3/Dosovitskiy et al. - 2017 - CARLA An Open Urban Driving Simulator.pdf;/home/mo/Zotero/storage/AR6TGJBA/1711.html},
  annote = {Comment: Published at the 1st Conference on Robot Learning (CoRL)}
}

@inproceedings{wolfLearningHowDrive2017,
  title = {Learning {{How}} to {{Drive}} in a {{Real World Simulation}} with {{Deep Q}}-{{Networks}}},
  abstract = {We present a reinforcement learning approach using Deep Q-Networks to steer a vehicle in a 3D physics simulation. Relying solely on camera image input the approach directly learns steering the vehicle in an end-to-end manner. The system is able to learn human driving behavior without the need of any labeled training data. An action-based reward function is proposed, which is motivated by a potential use in real world reinforcement learning scenarios. Compared to a na\textasciidieresis\i{}vena\textasciidieresis\i{}ve distance-based reward function, it improves the overall driving behavior of the vehicle agent. The agent is even able to reach comparable to human driving performance on a previously unseen track in our simulation environment.},
  doi = {10.1109/IVS.2017.7995727},
  author = {Wolf, Peter and Hubschneider, Christian and Weber, Michael and Bauer, Andr{\'e} and H{\"a}rtl, Jonathan and D{\"u}rr, Fabian and Z{\"o}llner, J},
  month = jun,
  year = {2017},
  file = {/home/mo/Zotero/storage/JXRMZAUQ/Wolf et al. - 2017 - Learning How to Drive in a Real World Simulation w.pdf}
}

@article{khanCarRacingUsing,
  title = {Car {{Racing}} Using {{Reinforcement Learning}}},
  abstract = {The aim of this project is to develop a generalizable system that can learn using signals generated only from its environment to optimize actions in the Car Racing game. Only pixel values and rewards provided by the environment are used to learn an optimal policy with a generalizable solution. Here we report on our work using deep convolutional neural nets and asynchronous advantage actor-critic methods to tackle this problem. Using these methods, we were able obtain an average reward of 652.29 over 100 episodes. For reference, the oracle for this project achieves an average reward of 837.76 over 100 episodes. Our results (oguzelibol's algorithm) and the solution we consider the oracle (ceobillionaire's algorithm) can be viewed at https://gym.openai.com/envs/CarRacing-v0.},
  language = {en},
  author = {Khan, M A Farhan},
  pages = {9},
  file = {/home/mo/Zotero/storage/QIGU6AAS/Khan - Car Racing using Reinforcement Learning.pdf}
}

@article{mnihAsynchronousMethodsDeep2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.01783},
  primaryClass = {cs},
  title = {Asynchronous {{Methods}} for {{Deep Reinforcement Learning}}},
  abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  language = {en},
  journal = {arXiv:1602.01783 [cs]},
  author = {Mnih, Volodymyr and Badia, Adri{\`a} Puigdom{\`e}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  month = feb,
  year = {2016},
  keywords = {Computer Science - Machine Learning},
  file = {/home/mo/Zotero/storage/ENSTJ42W/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learni.pdf}
}

@incollection{duanOneShotImitationLearning2017,
  title = {One-{{Shot Imitation Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  publisher = {{Curran Associates, Inc.}},
  author = {Duan, Yan and Andrychowicz, Marcin and Stadie, Bradly and Jonathan Ho, OpenAI and Schneider, Jonas and Sutskever, Ilya and Abbeel, Pieter and Zaremba, Wojciech},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {1087--1098},
  file = {/home/mo/Zotero/storage/Q49YJ3EN/Duan et al. - 2017 - One-Shot Imitation Learning.pdf;/home/mo/Zotero/storage/5USR8J2M/6709-one-shot-imitation-learning.html}
}

@article{liMetaSGDLearningLearn2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1707.09835},
  primaryClass = {cs},
  title = {Meta-{{SGD}}: {{Learning}} to {{Learn Quickly}} for {{Few}}-{{Shot Learning}}},
  shorttitle = {Meta-{{SGD}}},
  abstract = {Few-shot learning is challenging for learning algorithms that learn each task in isolation and from scratch. In contrast, meta-learning learns from many related tasks a meta-learner that can learn a new task more accurately and faster with fewer examples, where the choice of meta-learners is crucial. In this paper, we develop Meta-SGD, an SGD-like, easily trainable meta-learner that can initialize and adapt any differentiable learner in just one step, on both supervised learning and reinforcement learning. Compared to the popular meta-learner LSTM, Meta-SGD is conceptually simpler, easier to implement, and can be learned more efficiently. Compared to the latest meta-learner MAML, Meta-SGD has a much higher capacity by learning to learn not just the learner initialization, but also the learner update direction and learning rate, all in a single meta-learning process. Meta-SGD shows highly competitive performance for few-shot learning on regression, classification, and reinforcement learning.},
  language = {en},
  journal = {arXiv:1707.09835 [cs]},
  author = {Li, Zhenguo and Zhou, Fengwei and Chen, Fei and Li, Hang},
  month = jul,
  year = {2017},
  keywords = {Computer Science - Machine Learning},
  file = {/home/mo/Zotero/storage/PB8B3T72/Li et al. - 2017 - Meta-SGD Learning to Learn Quickly for Few-Shot L.pdf},
  annote = {Comment: reinforcement learning included, 20-way classification on MiniImagenet included}
}

@article{tangGridbotAutonomousRobot2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1807.02155},
  title = {Gridbot: {{An}} Autonomous Robot Controlled by a {{Spiking Neural Network}} Mimicking the Brain's Navigational System},
  shorttitle = {Gridbot},
  abstract = {It is true that the "best" neural network is not necessarily the one with the most "brain-like" behavior. Understanding biological intelligence, however, is a fundamental goal for several distinct disciplines. Translating our understanding of intelligence to machines is a fundamental problem in robotics. Propelled by new advancements in Neuroscience, we developed a spiking neural network (SNN) that draws from mounting experimental evidence that a number of individual neurons is associated with spatial navigation. By following the brain's structure, our model assumes no initial all-to-all connectivity, which could inhibit its translation to a neuromorphic hardware, and learns an uncharted territory by mapping its identified components into a limited number of neural representations, through spike-timing dependent plasticity (STDP). In our ongoing effort to employ a bioinspired SNN-controlled robot to real-world spatial mapping applications, we demonstrate here how an SNN may robustly control an autonomous robot in mapping and exploring an unknown environment, while compensating for its own intrinsic hardware imperfections, such as partial or total loss of visual input.},
  journal = {Proceedings of the International Conference on Neuromorphic Systems  - ICONS '18},
  doi = {10.1145/3229884.3229888},
  author = {Tang, Guangzhi and Michmizos, Konstantinos P.},
  year = {2018},
  keywords = {Computer Science - Artificial Intelligence,Quantitative Biology - Neurons and Cognition},
  pages = {1-8},
  file = {/home/mo/Zotero/storage/BTB8HYRE/Tang and Michmizos - 2018 - Gridbot An autonomous robot controlled by a Spiki.pdf;/home/mo/Zotero/storage/UEW7PF2C/1807.html},
  annote = {Comment: 8 pages, 3 Figures, International Conference on Neuromorphic Systems (ICONS 2018)}
}

@article{botvinickReinforcementLearningFast2019,
  title = {Reinforcement {{Learning}}, {{Fast}} and {{Slow}}},
  volume = {23},
  issn = {1879-307X},
  abstract = {Deep reinforcement learning (RL) methods have driven impressive advances in artificial intelligence in recent years, exceeding human performance in domains ranging from Atari to Go to no-limit poker. This progress has drawn the attention of cognitive scientists interested in understanding human learning. However, the concern has been raised that deep RL may be too sample-inefficient - that is, it may simply be too slow - to provide a plausible model of how humans learn. In the present review, we counter this critique by describing recently developed techniques that allow deep RL to operate more nimbly, solving problems much more quickly than previous methods. Although these techniques were developed in an AI context, we propose that they may have rich implications for psychology and neuroscience. A key insight, arising from these AI methods, concerns the fundamental connection between fast RL and slower, more incremental forms of learning.},
  language = {eng},
  number = {5},
  journal = {Trends in Cognitive Sciences},
  doi = {10.1016/j.tics.2019.02.006},
  author = {Botvinick, Matthew and Ritter, Sam and Wang, Jane X. and {Kurth-Nelson}, Zeb and Blundell, Charles and Hassabis, Demis},
  month = may,
  year = {2019},
  pages = {408-422},
  file = {/home/mo/Zotero/storage/FB8RG8W8/Botvinick et al. - 2019 - Reinforcement Learning, Fast and Slow.pdf},
  pmid = {31003893}
}

@article{lansdellLearningtolearn2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1811.00231},
  primaryClass = {cs, q-bio},
  title = {Towards Learning-to-Learn},
  abstract = {In good old-fashioned artificial intelligence (GOFAI), humans specified systems that solved problems. Much of the recent progress in AI has come from replacing human insights by learning. However, learning itself is still usually built by humans -- specifically the choice that parameter updates should follow the gradient of a cost function. Yet, in analogy with GOFAI, there is no reason to believe that humans are particularly good at defining such learning systems: we may expect learning itself to be better if we learn it. Recent research in machine learning has started to realize the benefits of that strategy. We should thus expect this to be relevant for neuroscience: how could the correct learning rules be acquired? Indeed, cognitive science has long shown that humans learn-to-learn, which is potentially responsible for their impressive learning abilities. Here we discuss ideas across machine learning, neuroscience, and cognitive science that matter for the principle of learning-to-learn.},
  journal = {arXiv:1811.00231 [cs, q-bio]},
  author = {Lansdell, Benjamin James and Kording, Konrad Paul},
  month = nov,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition},
  file = {/home/mo/Zotero/storage/7VXBDN8Y/Lansdell and Kording - 2018 - Towards learning-to-learn.pdf;/home/mo/Zotero/storage/SUI8TEA2/1811.html},
  annote = {Comment: 8 pages, 1 figure}
}

@article{wulfmeierMutualAlignmentTransfer2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1707.07907},
  primaryClass = {cs},
  title = {Mutual {{Alignment Transfer Learning}}},
  abstract = {Training robots for operation in the real world is a complex, time consuming and potentially expensive task. Despite significant success of reinforcement learning in games and simulations, research in real robot applications has not been able to match similar progress. While sample complexity can be reduced by training policies in simulation, such policies can perform sub-optimally on the real platform given imperfect calibration of model dynamics. We present an approach -- supplemental to fine tuning on the real robot -- to further benefit from parallel access to a simulator during training and reduce sample requirements on the real robot. The developed approach harnesses auxiliary rewards to guide the exploration for the real world agent based on the proficiency of the agent in simulation and vice versa. In this context, we demonstrate empirically that the reciprocal alignment for both agents provides further benefit as the agent in simulation can adjust to optimize its behaviour for states commonly visited by the real-world agent.},
  journal = {arXiv:1707.07907 [cs]},
  author = {Wulfmeier, Markus and Posner, Ingmar and Abbeel, Pieter},
  month = jul,
  year = {2017},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/mo/Zotero/storage/U8FXB6KB/Wulfmeier et al. - 2017 - Mutual Alignment Transfer Learning.pdf;/home/mo/Zotero/storage/NBE7CGH3/1707.html}
}

@article{chenLearningLearnGradient2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.03824},
  primaryClass = {cs, stat},
  title = {Learning to {{Learn}} without {{Gradient Descent}} by {{Gradient Descent}}},
  abstract = {We learn recurrent neural network optimizers trained on simple synthetic functions by gradient descent. We show that these learned optimizers exhibit a remarkable degree of transfer in that they can be used to efficiently optimize a broad range of derivative-free black-box functions, including Gaussian process bandits, simple control objectives, global optimization benchmarks and hyper-parameter tuning tasks. Up to the training horizon, the learned optimizers learn to trade-off exploration and exploitation, and compare favourably with heavily engineered Bayesian optimization packages for hyper-parameter tuning.},
  journal = {arXiv:1611.03824 [cs, stat]},
  author = {Chen, Yutian and Hoffman, Matthew W. and Colmenarejo, Sergio Gomez and Denil, Misha and Lillicrap, Timothy P. and Botvinick, Matt and {de Freitas}, Nando},
  month = nov,
  year = {2016},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/mo/Zotero/storage/FNCAVGHP/Chen et al. - 2016 - Learning to Learn without Gradient Descent by Grad.pdf;/home/mo/Zotero/storage/58TCELX7/1611.html},
  annote = {Comment: Accepted by ICML 2017. Previous version "Learning to Learn for Global Optimization of Black Box Functions" was published in the Deep Reinforcement Learning Workshop, NIPS 2016}
}

@article{kaiserEmbodiedNeuromorphicVision2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1904.04805},
  primaryClass = {cs},
  title = {Embodied {{Neuromorphic Vision}} with {{Event}}-{{Driven Random Backpropagation}}},
  abstract = {Spike-based communication between biological neurons is sparse and unreliable. This enables the brain to process visual information from the eyes efficiently. Taking inspiration from biology, artificial spiking neural networks coupled with silicon retinas attempt to model these computations. Recent findings in machine learning allowed the derivation of a family of powerful synaptic plasticity rules approximating backpropagation for spiking networks. Are these rules capable of processing real-world visual sensory data? In this paper, we evaluate the performance of Event-Driven Random Back-Propagation (eRBP) at learning representations from event streams provided by a Dynamic Vision Sensor (DVS). First, we show that eRBP matches state-of-the-art performance on the DvsGesture dataset with the addition of a simple covert attention mechanism. By remapping visual receptive fields relatively to the center of the motion, this attention mechanism provides translation invariance at low computational cost compared to convolutions. Second, we successfully integrate eRBP in a real robotic setup, where a robotic arm grasps objects according to detected visual affordances. In this setup, visual information is actively sensed by a DVS mounted on a robotic head performing microsaccadic eye movements. We show that our method classifies affordances within 100ms after microsaccade onset, which is comparable to human performance reported in behavioral study. Our results suggest that advances in neuromorphic technology and plasticity rules enable the development of autonomous robots operating at high speed and low energy consumption.},
  journal = {arXiv:1904.04805 [cs]},
  author = {Kaiser, Jacques and Friedrich, Alexander and Tieck, J. Camilo Vasquez and Reichard, Daniel and Roennau, Arne and Neftci, Emre and Dillmann, R{\"u}diger},
  month = apr,
  year = {2019},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/mo/Zotero/storage/YHZHHSE2/Kaiser et al. - 2019 - Embodied Neuromorphic Vision with Event-Driven Ran.pdf;/home/mo/Zotero/storage/MSRNKRPJ/1904.html},
  annote = {Comment: v2: title update, better plots and wordings. 8 pages, 9 figures, 1 table, video: https://neurorobotics-files.net/index.php/s/sBQzWFrBPoH9Dx7}
}

@article{ponghiranReinforcementLearningLowComplexity2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1906.01695},
  primaryClass = {cs, stat},
  title = {Reinforcement {{Learning}} with {{Low}}-{{Complexity Liquid State Machines}}},
  abstract = {We propose reinforcement learning on simple networks consisting of random connections of spiking neurons (both recurrent and feed-forward) that can learn complex tasks with very little trainable parameters. Such sparse and randomly interconnected recurrent spiking networks exhibit highly non-linear dynamics that transform the inputs into rich high-dimensional representations based on past context. The random input representations can be efficiently interpreted by an output (or readout) layer with trainable parameters. Systematic initialization of the random connections and training of the readout layer using Q-learning algorithm enable such small random spiking networks to learn optimally and achieve the same learning efficiency as humans on complex reinforcement learning tasks like Atari games. The spike-based approach using small random recurrent networks provides a computationally efficient alternative to state-of-the-art deep reinforcement learning networks with several layers of trainable parameters. The low-complexity spiking networks can lead to improved energy efficiency in event-driven neuromorphic hardware for complex reinforcement learning tasks.},
  journal = {arXiv:1906.01695 [cs, stat]},
  author = {Ponghiran, Wachirawit and Srinivasan, Gopalakrishnan and Roy, Kaushik},
  month = jun,
  year = {2019},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/mo/Zotero/storage/V6FY2QAN/Ponghiran et al. - 2019 - Reinforcement Learning with Low-Complexity Liquid .pdf;/home/mo/Zotero/storage/ED72IZ3V/1906.html},
  annote = {Comment: 6 figures}
}

@article{schweighoferMetalearningReinforcementLearning2003,
  title = {Meta-Learning in {{Reinforcement Learning}}},
  volume = {16},
  issn = {0893-6080},
  abstract = {Meta-parameters in reinforcement learning should be tuned to the environmental dynamics and the animal performance. Here, we propose a biologically plausible meta-reinforcement learning algorithm for tuning these meta-parameters in a dynamic, adaptive manner. We tested our algorithm in both a simulation of a Markov decision task and in a non-linear control task. Our results show that the algorithm robustly finds appropriate meta-parameter values, and controls the meta-parameter time course, in both static and dynamic environments. We suggest that the phasic and tonic components of dopamine neuron firing can encode the signal required for meta-learning of reinforcement learning.},
  number = {1},
  journal = {Neural Networks},
  doi = {10.1016/S0893-6080(02)00228-9},
  author = {Schweighofer, Nicolas and Doya, Kenji},
  month = jan,
  year = {2003},
  keywords = {Dopamine,Dynamic environment,Meta-learning,Meta-parameters,Neuromodulation,Reinforcement learning,TD error},
  pages = {5-9},
  file = {/home/mo/Zotero/storage/2ILAY3CZ/Schweighofer and Doya - 2003 - Meta-learning in Reinforcement Learning.pdf;/home/mo/Zotero/storage/8CHLX463/S0893608002002289.html}
}

@article{pfeifferDeepLearningSpiking2018,
  title = {Deep {{Learning With Spiking Neurons}}: {{Opportunities}} and {{Challenges}}},
  volume = {12},
  issn = {1662-4548},
  shorttitle = {Deep {{Learning With Spiking Neurons}}},
  abstract = {Spiking neural networks (SNNs) are inspired by information processing in biology, where sparse and asynchronous binary signals are communicated and processed in a massively parallel fashion. SNNs on neuromorphic hardware exhibit favorable properties such as low power consumption, fast inference, and event-driven information processing. This makes them interesting candidates for the efficient implementation of deep neural networks, the method of choice for many machine learning tasks. In this review, we address the opportunities that deep spiking networks offer and investigate in detail the challenges associated with training SNNs in a way that makes them competitive with conventional deep learning, but simultaneously allows for efficient mapping to hardware. A wide range of training methods for SNNs is presented, ranging from the conversion of conventional deep networks into SNNs, constrained training before conversion, spiking variants of backpropagation, and biologically motivated variants of STDP. The goal of our review is to define a categorization of SNN training methods, and summarize their advantages and drawbacks. We further discuss relationships between SNNs and binary networks, which are becoming popular for efficient digital hardware implementation. Neuromorphic hardware platforms have great potential to enable deep spiking networks in real-world applications. We compare the suitability of various neuromorphic systems that have been developed over the past years, and investigate potential use cases. Neuromorphic approaches and conventional machine learning should not be considered simply two solutions to the same classes of problems, instead it is possible to identify and exploit their task-specific advantages. Deep SNNs offer great opportunities to work with new types of event-based sensors, exploit temporal codes and local on-chip learning, and we have so far just scratched the surface of realizing these advantages in practical applications.},
  journal = {Frontiers in Neuroscience},
  doi = {10.3389/fnins.2018.00774},
  author = {Pfeiffer, Michael and Pfeil, Thomas},
  month = oct,
  year = {2018},
  file = {/home/mo/Zotero/storage/SI97ECUL/Pfeiffer and Pfeil - 2018 - Deep Learning With Spiking Neurons Opportunities .pdf},
  pmid = {30410432},
  pmcid = {PMC6209684}
}

@article{tavanaeiDeepLearningSpiking2019,
  title = {Deep Learning in Spiking Neural Networks},
  volume = {111},
  issn = {0893-6080},
  abstract = {In recent years, deep learning has revolutionized the field of machine learning, for computer vision in particular. In this approach, a deep (multilayer) artificial neural network (ANN) is trained, most often in a supervised manner using backpropagation. Vast amounts of labeled training examples are required, but the resulting classification accuracy is truly impressive, sometimes outperforming humans. Neurons in an ANN are characterized by a single, static, continuous-valued activation. Yet biological neurons use discrete spikes to compute and transmit information, and the spike times, in addition to the spike rates, matter. Spiking neural networks (SNNs) are thus more biologically realistic than ANNs, and are arguably the only viable option if one wants to understand how the brain computes at the neuronal description level. The spikes of biological neurons are sparse in time and space, and event-driven. Combined with bio-plausible local learning rules, this makes it easier to build low-power, neuromorphic hardware for SNNs. However, training deep SNNs remains a challenge. Spiking neurons' transfer function is usually non-differentiable, which prevents using backpropagation. Here we review recent supervised and unsupervised methods to train deep SNNs, and compare them in terms of accuracy and computational cost. The emerging picture is that SNNs still lag behind ANNs in terms of accuracy, but the gap is decreasing, and can even vanish on some tasks, while SNNs typically require many fewer operations and are the better candidates to process spatio-temporal data.},
  journal = {Neural Networks},
  doi = {10.1016/j.neunet.2018.12.002},
  author = {Tavanaei, Amirhossein and Ghodrati, Masoud and Kheradpisheh, Saeed Reza and Masquelier, Timoth{\'e}e and Maida, Anthony},
  month = mar,
  year = {2019},
  keywords = {Biological plausibility,Deep learning,Machine learning,Power-efficient architecture,Spiking neural network},
  pages = {47-63},
  file = {/home/mo/Zotero/storage/BWQXVANU/Tavanaei et al. - 2019 - Deep learning in spiking neural networks.pdf;/home/mo/Zotero/storage/C6VK3R2J/S0893608018303332.html}
}

@incollection{florianReinforcementLearningSpiking2012,
  address = {{Boston, MA}},
  title = {Reinforcement {{Learning}} in {{Spiking Neural Networks}}},
  isbn = {978-1-4419-1427-9 978-1-4419-1428-6},
  language = {en},
  booktitle = {Encyclopedia of the {{Sciences}} of {{Learning}}},
  publisher = {{Springer US}},
  author = {Florian, R{\u a}zvan V.},
  editor = {Seel, Norbert M.},
  year = {2012},
  pages = {2802-2803},
  doi = {10.1007/978-1-4419-1428-6_1713}
}

@article{evansReinforcementLearningNeurally2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.06096},
  primaryClass = {cs},
  title = {Reinforcement {{Learning}} in a {{Neurally Controlled Robot Using Dopamine Modulated STDP}}},
  abstract = {Recent work has shown that dopamine-modulated STDP can solve many of the issues associated with reinforcement learning, such as the distal reward problem. Spiking neural networks provide a useful technique in implementing reinforcement learning in an embodied context as they can deal with continuous parameter spaces and as such are better at generalizing the correct behaviour to perform in a given context. In this project we implement a version of DA-modulated STDP in an embodied robot on a food foraging task. Through simulated dopaminergic neurons we show how the robot is able to learn a sequence of behaviours in order to achieve a food reward. In tests the robot was able to learn food-attraction behaviour, and subsequently unlearn this behaviour when the environment changed, in all 50 trials. Moreover we show that the robot is able to operate in an environment whereby the optimal behaviour changes rapidly and so the agent must constantly relearn. In a more complex environment, consisting of food-containers, the robot was able to learn food-container attraction in 95\% of trials, despite the large temporal distance between the correct behaviour and the reward. This is achieved by shifting the dopamine response from the primary stimulus (food) to the secondary stimulus (food-container). Our work provides insights into the reasons behind some observed biological phenomena, such as the bursting behaviour observed in dopaminergic neurons. As well as demonstrating how spiking neural network controlled robots are able to solve a range of reinforcement learning tasks.},
  journal = {arXiv:1502.06096 [cs]},
  author = {Evans, Richard},
  month = feb,
  year = {2015},
  keywords = {Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics},
  file = {/home/mo/Zotero/storage/I5AEF644/Evans - 2015 - Reinforcement Learning in a Neurally Controlled Ro.pdf;/home/mo/Zotero/storage/KNV6BT54/1502.html}
}

@inproceedings{florianReinforcementLearningAlgorithm2005,
  title = {A Reinforcement Learning Algorithm for Spiking Neural Networks, In},
  abstract = {networks},
  booktitle = {Eds, `{{Proceedings}} of the {{Seventh International Symposium}} on {{Symbolic}} and {{Numeric Algorithms}} for {{Scientific Computing}} ({{SYNASC}} 2005)', {{IEEE Computer Society}}, {{Los Alamitos}}, {{CA}}},
  author = {Florian, R{\u a}zvan V.},
  year = {2005},
  pages = {299--306},
  file = {/home/mo/Zotero/storage/IDT3C7JJ/Florian - 2005 - A reinforcement learning algorithm for spiking neu.pdf;/home/mo/Zotero/storage/H5PV36Q7/summary.html}
}

@incollection{florianReinforcementLearningSpiking2012a,
  address = {{Boston, MA}},
  title = {Reinforcement {{Learning}} in {{Spiking Neural Networks}}},
  isbn = {978-1-4419-1428-6},
  language = {en},
  booktitle = {Encyclopedia of the {{Sciences}} of {{Learning}}},
  publisher = {{Springer US}},
  author = {Florian, R{\u a}zvan V.},
  editor = {Seel, Norbert M.},
  year = {2012},
  pages = {2802-2803},
  doi = {10.1007/978-1-4419-1428-6_1713}
}

@inproceedings{florianReinforcementLearningAlgorithm2005a,
  address = {{Timisoara, Romania}},
  title = {A Reinforcement Learning Algorithm for Spiking Neural Networks},
  isbn = {978-0-7695-2453-5},
  abstract = {The paper presents a new reinforcement learning mechanism for spiking neural networks. The algorithm is derived for networks of stochastic integrate-and-fire neurons, but it can be also applied to generic spiking neural networks. Learning is achieved by synaptic changes that depend on the firing of pre- and postsynaptic neurons, and that are modulated with a global reinforcement signal. The efficacy of the algorithm is verified in a biologically-inspired experiment, featuring a simulated worm that searches for food. Our model recovers a form of neural plasticity experimentally observed in animals, combining spike-timing-dependent synaptic changes of one sign with non-associative synaptic changes of the opposite sign determined by presynaptic spikes. The model also predicts that the time constant of spike-timing-dependent synaptic changes is equal to the membrane time constant of the neuron, in agreement with experimental observations in the brain. This study also led to the discovery of a biologically-plausible reinforcement learning mechanism that works by modulating spike-timing-dependent plasticity (STDP) with a global reward signal.},
  language = {en},
  booktitle = {Seventh {{International Symposium}} on {{Symbolic}} and {{Numeric Algorithms}} for {{Scientific Computing}} ({{SYNASC}}'05)},
  publisher = {{IEEE}},
  doi = {10.1109/SYNASC.2005.13},
  author = {Florian, R.V.},
  year = {2005},
  pages = {8 pp.},
  file = {/home/mo/Zotero/storage/9RLYP5QQ/Florian - 2005 - A reinforcement learning algorithm for spiking neu.pdf}
}

@article{florianReinforcementLearningModulation2007,
  title = {Reinforcement {{Learning Through Modulation}} of {{Spike}}-{{Timing}}-{{Dependent Synaptic Plasticity}}},
  volume = {19},
  issn = {0899-7667},
  abstract = {The persistent modification of synaptic efficacy as a function of the relative timing of pre- and postsynaptic spikes is a phenomenon known as spike-timing-dependent plasticity (STDP). Here we show that the modulation of STDP by a global reward signal leads to reinforcement learning. We first derive analytically learning rules involving reward-modulated spike-timing-dependent synaptic and intrinsic plasticity, by applying a reinforcement learning algorithm to the stochastic spike response model of spiking neurons. These rules have several features common to plasticity mechanisms experimentally found in the brain. We then demonstrate in simulations of networks of integrate-and-fire neurons the efficacy of two simple learning rules involving modulated STDP. One rule is a direct extension of the standard STDP model (modulated STDP), and the other one involves an eligibility trace stored at each synapse that keeps a decaying memory of the relationships between the recent pairs of pre- and postsynaptic spike pairs (modulated STDP with eligibility trace). This latter rule permits learning even if the reward signal is delayed. The proposed rules are able to solve the XOR problem with both rate coded and temporally coded input and to learn a target output firing-rate pattern. These learning rules are biologically plausible, may be used for training generic artificial spiking neural networks, regardless of the neural model used, and suggest the experimental investigation in animals of the existence of reward-modulated STDP.},
  number = {6},
  journal = {Neural Computation},
  doi = {10.1162/neco.2007.19.6.1468},
  author = {Florian, R{\u a}zvan V.},
  month = apr,
  year = {2007},
  pages = {1468-1502},
  file = {/home/mo/Zotero/storage/FJN7AWIT/neco.2007.19.6.html}
}

@article{seungLearningSpikingNeural2003,
  title = {Learning in {{Spiking Neural Networks}} by {{Reinforcement}} of {{Stochastic Synaptic Transmission}}},
  volume = {40},
  issn = {0896-6273},
  abstract = {It is well-known that chemical synaptic transmission is an unreliable process, but the function of such unreliability remains unclear. Here I consider the hypothesis that the randomness of synaptic transmission is harnessed by the brain for learning, in analogy to the way that genetic mutation is utilized by Darwinian evolution. This is possible if synapses are ``hedonistic,'' responding to a global reward signal by increasing their probabilities of vesicle release or failure, depending on which action immediately preceded reward. Hedonistic synapses learn by computing a stochastic approximation to the gradient of the average reward. They are compatible with synaptic dynamics such as short-term facilitation and depression and with the intricacies of dendritic integration and action potential generation. A network of hedonistic synapses can be trained to perform a desired computation by administering reward appropriately, as illustrated here through numerical simulations of integrate-and-fire model neurons.},
  number = {6},
  journal = {Neuron},
  doi = {10.1016/S0896-6273(03)00761-X},
  author = {Seung, H. Sebastian},
  month = dec,
  year = {2003},
  pages = {1063-1073},
  file = {/home/mo/Zotero/storage/UYKCTSB3/Seung - 2003 - Learning in Spiking Neural Networks by Reinforceme.pdf;/home/mo/Zotero/storage/M2GEUNVQ/S089662730300761X.html}
}

@article{sboevSpikingNeuralNetwork2018,
  series = {Postproceedings of the 9th {{Annual International Conference}} on {{Biologically Inspired Cognitive Architectures}}, {{BICA}} 2018 ({{Ninth Annual Meeting}} of the {{BICA Society}}), Held {{August}} 22-24, 2018 in {{Prague}}, {{Czech Republic}}},
  title = {Spiking Neural Network Reinforcement Learning Method Based on Temporal Coding and {{STDP}}},
  volume = {145},
  issn = {1877-0509},
  abstract = {A method to train a spiking network to solve a classification task using Spike-Timing-Dependent Plasticity is proposed. Learning is based on the ability of STDP to memorize repeating spike patterns: earliest spikes of pattern contribute to output spike and those weights stay high, while others weights falls down to zero. The output neurons are provided with information on the classes by stimulating the neuron corresponding to the desired class to fire an early spike. The network is single-layer, with competition introduced by inhibitory interconnections. The network consists of leaky integrate-and-fire neurons tuned to provide one spike per pattern. Input data is encoded by Gaussian receptive fields, where earliest spikes contains the most information. The learning method is tested on Fisher's Iris and Wisconsin Breast Cancer datasets, and results compared to Support Vector Machines, Random Forest and formal neural networks with Adam optimizer.},
  journal = {Procedia Computer Science},
  doi = {10.1016/j.procs.2018.11.107},
  author = {Sboev, Alexander and Vlasov, Danila and Rybka, Roman and Serenko, Alexey},
  month = jan,
  year = {2018},
  keywords = {classification,reinforcement learning,spike-timing-dependent plasticity,spiking neural network,temporal coding},
  pages = {458-463},
  file = {/home/mo/Zotero/storage/BQI9SBRE/Sboev et al. - 2018 - Spiking neural network reinforcement learning meth.pdf;/home/mo/Zotero/storage/GKL4GYLQ/S1877050918323950.html}
}

@article{schmidhuberDeepLearningNeural2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1404.7828},
  title = {Deep {{Learning}} in {{Neural Networks}}: {{An Overview}}},
  volume = {61},
  issn = {08936080},
  shorttitle = {Deep {{Learning}} in {{Neural Networks}}},
  abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
  journal = {Neural Networks},
  doi = {10.1016/j.neunet.2014.09.003},
  author = {Schmidhuber, Juergen},
  month = jan,
  year = {2015},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  pages = {85-117},
  file = {/home/mo/Zotero/storage/GT7935I5/Schmidhuber - 2015 - Deep Learning in Neural Networks An Overview.pdf;/home/mo/Zotero/storage/JZAX8RLT/1404.html},
  annote = {Comment: 88 pages, 888 references}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  volume = {521},
  copyright = {2015 Nature Publishing Group},
  issn = {1476-4687},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  language = {en},
  number = {7553},
  journal = {Nature},
  doi = {10.1038/nature14539},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  month = may,
  year = {2015},
  pages = {436-444},
  file = {/home/mo/Zotero/storage/RD56TKLL/nature14539.html}
}

@article{lecunDeepLearning2015a,
  title = {Deep Learning},
  volume = {521},
  issn = {0028-0836, 1476-4687},
  language = {en},
  number = {7553},
  journal = {Nature},
  doi = {10.1038/nature14539},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  month = may,
  year = {2015},
  pages = {436-444},
  file = {/home/mo/Zotero/storage/MX692XTJ/LeCun et al. - 2015 - Deep learning.pdf}
}

@article{05SpikingNeuralNetworks,
  title = {05\_{{SpikingNeuralNetworks}}\_1},
  language = {en},
  pages = {43},
  file = {/home/mo/Zotero/storage/3HQECCPF/05_SpikingNeuralNetworks_1.pdf}
}

@article{hesselRainbowCombiningImprovements2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.02298},
  primaryClass = {cs},
  title = {Rainbow: {{Combining Improvements}} in {{Deep Reinforcement Learning}}},
  shorttitle = {Rainbow},
  abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
  journal = {arXiv:1710.02298 [cs]},
  author = {Hessel, Matteo and Modayil, Joseph and {van Hasselt}, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  month = oct,
  year = {2017},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/mo/Zotero/storage/C8GTL5YL/Hessel et al. - 2017 - Rainbow Combining Improvements in Deep Reinforcem.pdf;/home/mo/Zotero/storage/2EP4MKE3/1710.html},
  annote = {Comment: Under review as a conference paper at AAAI 2018}
}

@article{hardtTrainFasterGeneralize2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1509.01240},
  primaryClass = {cs, math, stat},
  title = {Train Faster, Generalize Better: {{Stability}} of Stochastic Gradient Descent},
  shorttitle = {Train Faster, Generalize Better},
  abstract = {We show that parametric models trained by a stochastic gradient method (SGM) with few iterations have vanishing generalization error. We prove our results by arguing that SGM is algorithmically stable in the sense of Bousquet and Elisseeff. Our analysis only employs elementary tools from convex and continuous optimization. We derive stability bounds for both convex and non-convex optimization under standard Lipschitz and smoothness assumptions. Applying our results to the convex case, we provide new insights for why multiple epochs of stochastic gradient methods generalize well in practice. In the non-convex case, we give a new interpretation of common practices in neural networks, and formally show that popular techniques for training large deep models are indeed stability-promoting. Our findings conceptually underscore the importance of reducing training time beyond its obvious benefit.},
  journal = {arXiv:1509.01240 [cs, math, stat]},
  author = {Hardt, Moritz and Recht, Benjamin and Singer, Yoram},
  month = sep,
  year = {2015},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Mathematics - Optimization and Control},
  file = {/home/mo/Zotero/storage/UN7W8AJJ/Hardt et al. - 2015 - Train faster, generalize better Stability of stoc.pdf;/home/mo/Zotero/storage/89SUYZWE/1509.html}
}

@article{hochreiterLongShortTermMemory1997,
  title = {Long {{Short}}-{{Term Memory}}},
  volume = {9},
  issn = {0899-7667},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  number = {8},
  journal = {Neural Comput.},
  doi = {10.1162/neco.1997.9.8.1735},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  month = nov,
  year = {1997},
  pages = {1735--1780},
  file = {/home/mo/Zotero/storage/IJ9JF52T/2604.pdf}
}

@inproceedings{hochreiterLearningLearnUsing2001,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Learning to {{Learn Using Gradient Descent}}},
  isbn = {978-3-540-44668-2},
  abstract = {This paper introduces the application of gradient descent methods to meta-learning. The concept of ``meta-learning'', i.e. of a system that improves or discovers a learning algorithm, has been of interest in machine learning for decades because of its appealing applications. Previous meta-learning approaches have been based on evolutionary methods and, therefore, have been restricted to small models with few free parameters. We make meta-learning in large systems feasible by using recurrent neural networks with their attendant learning routines as meta-learning systems. Our system derived complex well performing learning algorithms from scratch. In this paper we also show that our approach performs non-stationary time series prediction.},
  language = {en},
  booktitle = {Artificial {{Neural Networks}} \textemdash{} {{ICANN}} 2001},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Hochreiter, Sepp and Younger, A. Steven and Conwell, Peter R.},
  editor = {Dorffner, Georg and Bischof, Horst and Hornik, Kurt},
  year = {2001},
  keywords = {Boolean Function,Gradient Descent,Hide Layer,Learning Algorithm,Turing Machine},
  pages = {87-94},
  file = {/home/mo/Zotero/storage/GQ5Z7369/Hochreiter et al. - 2001 - Learning to Learn Using Gradient Descent.pdf}
}

@book{schweighoferaNeuralNetworksLetter2002,
  title = {Neural {{Networks}} Letter {{Meta}}-Learning in {{Reinforcement Learning}}},
  abstract = {Meta-parameters in reinforcement learning should be tuned to the environmental dynamics and the animal performance. Here, we propose a biologically plausible meta-reinforcement learning algorithm for tuning these meta-parameters in a dynamic, adaptive manner. We tested our algorithm in both a simulation of a Markov decision task and in a non-linear control task. Our results show that the algorithm robustly finds appropriate meta-parameter values, and controls the meta-parameter time course, in both static and dynamic environments. We suggest that the phasic and tonic components of dopamine neuron firing can encode the signal required for meta-learning of reinforcement learning.},
  author = {Schweighofera, Nicolas and Doyaa, Kenji},
  year = {2002},
  file = {/home/mo/Zotero/storage/K6QHK8GQ/Schweighofera and Doyaa - 2002 - Neural Networks letter Meta-learning in Reinforcem.pdf;/home/mo/Zotero/storage/LGA9F8Z7/summary.html}
}

@incollection{xuDynamicMatrixRecovery2016,
  title = {Dynamic Matrix Recovery from Incomplete Observations under an Exact Low-Rank Constraint},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  publisher = {{Curran Associates, Inc.}},
  author = {Xu, Liangbei and Davenport, Mark},
  editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  year = {2016},
  pages = {3585--3593},
  file = {/home/mo/Zotero/storage/TMZP9NJS/6460-learning-to-learn-by-gradient-descent-by-gradient-descent.html}
}

@incollection{andrychowiczLearningLearnGradient2016,
  title = {Learning to Learn by Gradient Descent by Gradient Descent},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  publisher = {{Curran Associates, Inc.}},
  author = {Andrychowicz, Marcin and Denil, Misha and G{\'o}mez, Sergio and Hoffman, Matthew W and Pfau, David and Schaul, Tom and Shillingford, Brendan and {de Freitas}, Nando},
  editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  year = {2016},
  pages = {3981--3989},
  file = {/home/mo/Zotero/storage/PVLJDDRG/Andrychowicz et al. - 2016 - Learning to learn by gradient descent by gradient .pdf;/home/mo/Zotero/storage/JCWZZSHB/6461-learning-to-learn-by-gradient-descent-by-gradient-descent.html}
}

@incollection{duanOneShotImitationLearning2017a,
  title = {One-{{Shot Imitation Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  publisher = {{Curran Associates, Inc.}},
  author = {Duan, Yan and Andrychowicz, Marcin and Stadie, Bradly and Jonathan Ho, OpenAI and Schneider, Jonas and Sutskever, Ilya and Abbeel, Pieter and Zaremba, Wojciech},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {1087--1098},
  file = {/home/mo/Zotero/storage/Z45GZ7DY/Duan et al. - 2017 - One-Shot Imitation Learning.pdf;/home/mo/Zotero/storage/R7R6MZA2/6709-one-shot-imitation-learning.html}
}

@inproceedings{belloNeuralOptimizerSearch2017,
  series = {{{ICML}}'17},
  title = {Neural {{Optimizer Search}} with {{Reinforcement Learning}}},
  abstract = {We present an approach to automate the process of discovering optimization methods, with a focus on deep learning architectures. We train a Recurrent Neural Network controller to generate a string in a specific domain language that describes a mathematical update equation based on a list of primitive functions, such as the gradient, running average of the gradient, etc. The controller is trained with Reinforcement Learning to maximize the performance of a model after a few epochs. On CIFAR-10, our method discovers several update rules that are better than many commonly used optimizers, such as Adam, RMSProp, or SGD with and without Momentum on a ConvNet model. These optimizers can also be transferred to perform well on different neural network architectures, including Google's neural machine translation system.},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}} - {{Volume}} 70},
  publisher = {{JMLR.org}},
  author = {Bello, Irwan and Zoph, Barret and Vasudevan, Vijay and Le, Quoc V.},
  year = {2017},
  pages = {459--468},
  file = {/home/mo/Zotero/storage/3TJ9EC58/Bello et al. - 2017 - Neural Optimizer Search with Reinforcement Learnin.pdf}
}

@article{zophNeuralArchitectureSearch2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.01578},
  primaryClass = {cs},
  title = {Neural {{Architecture Search}} with {{Reinforcement Learning}}},
  abstract = {Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.},
  journal = {arXiv:1611.01578 [cs]},
  author = {Zoph, Barret and Le, Quoc V.},
  month = nov,
  year = {2016},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/mo/Zotero/storage/QTVECMTH/Zoph and Le - 2016 - Neural Architecture Search with Reinforcement Lear.pdf;/home/mo/Zotero/storage/NUV8BG98/1611.html}
}

@article{wangPrefrontalCortexMetareinforcement2018,
  title = {Prefrontal Cortex as a Meta-Reinforcement Learning System},
  volume = {21},
  issn = {1097-6256, 1546-1726},
  language = {en},
  number = {6},
  journal = {Nature Neuroscience},
  doi = {10.1038/s41593-018-0147-8},
  author = {Wang, Jane X. and {Kurth-Nelson}, Zeb and Kumaran, Dharshan and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z. and Hassabis, Demis and Botvinick, Matthew},
  month = jun,
  year = {2018},
  pages = {860-868},
  file = {/home/mo/Zotero/storage/5DN99M52/Wang et al. - 2018 - Prefrontal cortex as a meta-reinforcement learning.pdf}
}

@article{duanBenchmarkingDeepReinforcement,
  title = {Benchmarking {{Deep Reinforcement Learning}} for {{Continuous Control}}},
  abstract = {Recently, researchers have made significant progress combining the advances in deep learning for learning feature representations with reinforcement learning. Some notable examples include training agents to play Atari games based on raw pixel data and to acquire advanced manipulation skills using raw sensory inputs. However, it has been difficult to quantify progress in the domain of continuous control due to the lack of a commonly adopted benchmark. In this work, we present a benchmark suite of continuous control tasks, including classic tasks like cart-pole swing-up, tasks with very high state and action dimensionality such as 3D humanoid locomotion, tasks with partial observations, and tasks with hierarchical structure. We report novel findings based on the systematic evaluation of a range of implemented reinforcement learning algorithms. Both the benchmark and reference implementations are released at https://github.com/ rllab/rllab in order to facilitate experimental reproducibility and to encourage adoption by other researchers.},
  language = {en},
  author = {Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
  pages = {10},
  file = {/home/mo/Zotero/storage/3LSWJLCY/Duan et al. - Benchmarking Deep Reinforcement Learning for Conti.pdf}
}

@article{openaiLearningDexterousInHand2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1808.00177},
  primaryClass = {cs, stat},
  title = {Learning {{Dexterous In}}-{{Hand Manipulation}}},
  abstract = {We use reinforcement learning (RL) to learn dexterous in-hand manipulation policies which can perform vision-based object reorientation on a physical Shadow Dexterous Hand. The training is performed in a simulated environment in which we randomize many of the physical properties of the system like friction coefficients and an object's appearance. Our policies transfer to the physical robot despite being trained entirely in simulation. Our method does not rely on any human demonstrations, but many behaviors found in human manipulation emerge naturally, including finger gaiting, multi-finger coordination, and the controlled use of gravity. Our results were obtained using the same distributed RL system that was used to train OpenAI Five. We also include a video of our results: https://youtu.be/jwSbzNHGflM},
  journal = {arXiv:1808.00177 [cs, stat]},
  author = {OpenAI and Andrychowicz, Marcin and Baker, Bowen and Chociej, Maciek and Jozefowicz, Rafal and McGrew, Bob and Pachocki, Jakub and Petron, Arthur and Plappert, Matthias and Powell, Glenn and Ray, Alex and Schneider, Jonas and Sidor, Szymon and Tobin, Josh and Welinder, Peter and Weng, Lilian and Zaremba, Wojciech},
  month = aug,
  year = {2018},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Robotics},
  file = {/home/mo/Zotero/storage/XXZX2URV/OpenAI et al. - 2018 - Learning Dexterous In-Hand Manipulation.pdf;/home/mo/Zotero/storage/2IPCRKB8/1808.html},
  annote = {Comment: Making OpenAI the first author. We wish this paper to be cited as "Learning Dexterous In-Hand Manipulation" by OpenAI et al. We are replicating the approach from the physics community: arXiv:1812.06489}
}

@article{tobinDomainRandomizationTransferring2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.06907},
  primaryClass = {cs},
  title = {Domain {{Randomization}} for {{Transferring Deep Neural Networks}} from {{Simulation}} to the {{Real World}}},
  abstract = {Bridging the 'reality gap' that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to \$1.5\$cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.},
  journal = {arXiv:1703.06907 [cs]},
  author = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
  month = mar,
  year = {2017},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/mo/Zotero/storage/6WQ2ERJE/Tobin et al. - 2017 - Domain Randomization for Transferring Deep Neural .pdf;/home/mo/Zotero/storage/GP6BQWWR/1703.html},
  annote = {Comment: 8 pages, 7 figures. Submitted to 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2017)}
}

@article{zophLearningTransferableArchitectures2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1707.07012},
  primaryClass = {cs, stat},
  title = {Learning {{Transferable Architectures}} for {{Scalable Image Recognition}}},
  abstract = {Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the "NASNet search space") which enables transferability. In our experiments, we search for the best convolutional layer (or "cell") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named "NASNet architecture". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, NASNet achieves 2.4\% error rate, which is state-of-the-art. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7\% top-1 and 96.2\% top-5 on ImageNet. Our model is 1.2\% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28\% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74\% top-1 accuracy, which is 3.1\% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0\% achieving 43.1\% mAP on the COCO dataset.},
  journal = {arXiv:1707.07012 [cs, stat]},
  author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
  month = jul,
  year = {2017},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/mo/Zotero/storage/VHLQFS6U/Zoph et al. - 2017 - Learning Transferable Architectures for Scalable I.pdf;/home/mo/Zotero/storage/AC8JG5S3/1707.html}
}

@inproceedings{pengSimtoRealTransferRobotic2018,
  title = {Sim-to-{{Real Transfer}} of {{Robotic Control}} with {{Dynamics Randomization}}},
  abstract = {Simulations are attractive environments for training agents as they provide an abundant source of data and alleviate certain safety concerns during the training process. But the behaviours developed by agents in simulation are often specific to the characteristics of the simulator. Due to modeling error, strategies that are successful in simulation may not transfer to their real world counterparts. In this paper, we demonstrate a simple method to bridge this ``reality gap''. By randomizing the dynamics of the simulator during training, we are able to develop policies that are capable of adapting to very different dynamics, including ones that differ significantly from the dynamics on which the policies were trained. This adaptivity enables the policies to generalize to the dynamics of the real world without any training on the physical system. Our approach is demonstrated on an object pushing task using a robotic arm. Despite being trained exclusively in simulation, our policies are able to maintain a similar level of performance when deployed on a real robot, reliably moving an object to a desired location from random initial configurations. We explore the impact of various design decisions and show that the resulting policies are robust to significant calibration error.},
  booktitle = {2018 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  doi = {10.1109/ICRA.2018.8460528},
  author = {Peng, X. B. and Andrychowicz, M. and Zaremba, W. and Abbeel, P.},
  month = may,
  year = {2018},
  keywords = {Adaptation models,calibration,calibration error,Data models,dynamics randomization,mobile robots,multi-agent systems,multi-robot systems,robot dynamics,robotic arm,robotic control,Robots,Robustness,sim-to-real transfer,Task analysis,Training,training agents,training process,Trajectory},
  pages = {1-8},
  file = {/home/mo/Zotero/storage/L6MA7YS8/Peng et al. - 2018 - Sim-to-Real Transfer of Robotic Control with Dynam.pdf;/home/mo/Zotero/storage/CSKRQBNL/8460528.html}
}

@article{raviOPTIMIZATIONMODELFEWSHOT2017,
  title = {{{OPTIMIZATION AS A MODEL FOR FEW}}-{{SHOT LEARNING}}},
  abstract = {Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a classifier has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity classifiers requires many iterative steps over many examples to perform well. Here, we propose an LSTMbased meta-learner model to learn the exact optimization algorithm used to train another learner neural network classifier in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner (classifier) network that allows for quick convergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning.},
  language = {en},
  author = {Ravi, Sachin and Larochelle, Hugo},
  year = {2017},
  pages = {11},
  file = {/home/mo/Zotero/storage/BP6U9GJR/Ravi and Larochelle - 2017 - OPTIMIZATION AS A MODEL FOR FEW-SHOT LEARNING.pdf}
}

@article{mikolovRecurrentNeuralNetwork,
  title = {Recurrent {{Neural Network Based Language Model}}},
  abstract = {A new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. Results indicate that it is possible to obtain around 50\% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model. Speech recognition experiments show around 18\% reduction of word error rate on the Wall Street Journal task when comparing models trained on the same amount of data, and around 5\% on the much harder NIST RT05 task, even when the backoff model is trained on much more data than the RNN LM. We provide ample empirical evidence to suggest that connectionist language models are superior to standard n-gram techniques, except their high computational (training) complexity.},
  language = {en},
  author = {Mikolov, Tomas and Karafiat, Martin and Burget, Lukas and Cernocky, Jan and Khudanpur, Sanjeev},
  pages = {4},
  file = {/home/mo/Zotero/storage/IYWGPHRT/Mikolov et al. - Recurrent Neural Network Based Language Model.pdf}
}

@article{gravesEndtoEndSpeechRecognitionwith,
  title = {Towards {{End}}-to-{{End Speech Recognitionwith Recurrent Neural Networks}}},
  abstract = {This paper presents a speech recognition system that directly transcribes audio data with text, without requiring an intermediate phonetic representation. The system is based on a combination of the deep bidirectional LSTM recurrent neural network architecture and the Connectionist Temporal Classification objective function. A modification to the objective function is introduced that trains the network to minimise the expectation of an arbitrary transcription loss function. This allows a direct optimisation of the word error rate, even in the absence of a lexicon or language model. The system achieves a word error rate of 27.3\% on the Wall Street Journal corpus with no prior linguistic information, 21.9\% with only a lexicon of allowed words, and 8.2\% with a trigram language model. Combining the network with a baseline system further reduces the error rate to 6.7\%.},
  language = {en},
  author = {Graves, Alex and Jaitly, Navdeep},
  pages = {9},
  file = {/home/mo/Zotero/storage/9N72MQUP/Graves and Jaitly - Towards End-to-End Speech Recognitionwith Recurren.pdf}
}

@inproceedings{liuRecursiveRecurrentNeural2014,
  address = {{Baltimore, Maryland}},
  title = {A {{Recursive Recurrent Neural Network}} for {{Statistical Machine Translation}}},
  booktitle = {Proceedings of the 52nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.3115/v1/P14-1140},
  author = {Liu, Shujie and Yang, Nan and Li, Mu and Zhou, Ming},
  month = jun,
  year = {2014},
  pages = {1491--1500},
  file = {/home/mo/Zotero/storage/KHGPLP9L/Liu et al. - 2014 - A Recursive Recurrent Neural Network for Statistic.pdf}
}

@misc{UnderstandingLSTMNetworks,
  title = {Understanding {{LSTM Networks}} -- Colah's Blog},
  howpublished = {http://colah.github.io/posts/2015-08-Understanding-LSTMs/}
}

@misc{culurcielloFallRNNLSTM2018,
  title = {The Fall of {{RNN}} / {{LSTM}}},
  abstract = {We fell for Recurrent neural networks (RNN), Long-short term memory (LSTM), and all their variants. Now it is time to drop them!},
  journal = {Towards Data Science},
  howpublished = {https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0},
  author = {Culurciello, Eugenio},
  month = apr,
  year = {2018},
  file = {/home/mo/Zotero/storage/XD2EWP6J/the-fall-of-rnn-lstm-2d1594c74ce0.html}
}

@misc{UnderstandingLSTMNetworksa,
  title = {Understanding {{LSTM Networks}} -- Colah's Blog},
  howpublished = {http://colah.github.io/posts/2015-08-Understanding-LSTMs/}
}

@article{heDeepResidualLearning2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1512.03385},
  primaryClass = {cs},
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  journal = {arXiv:1512.03385 [cs]},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  month = dec,
  year = {2015},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/mo/Zotero/storage/7ERJSW5B/lecun2015lstm.pdf;/home/mo/Zotero/storage/9R25GUEG/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf;/home/mo/Zotero/storage/SHU9QNDQ/1512.html},
  annote = {Comment: Tech report}
}

@article{bengioLearningLongtermDependencies1994,
  title = {Learning Long-Term Dependencies with Gradient Descent Is Difficult},
  volume = {5},
  issn = {1045-9227},
  abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.{$<>$}},
  number = {2},
  journal = {IEEE Transactions on Neural Networks},
  doi = {10.1109/72.279181},
  author = {Bengio, Y. and Simard, P. and Frasconi, P.},
  month = mar,
  year = {1994},
  keywords = {Computer networks,Cost function,Delay effects,Discrete transforms,Displays,efficient learning,gradient descent,input/output sequence mapping,Intelligent networks,learning (artificial intelligence),long-term dependencies,Neural networks,Neurofeedback,numerical analysis,prediction problems,Production,production problems,recognition,recurrent neural nets,recurrent neural network training,Recurrent neural networks,temporal contingencies},
  pages = {157-166},
  file = {/home/mo/Zotero/storage/3EU5S967/tnn-94-gradient.pdf;/home/mo/Zotero/storage/75QISW9Q/279181.html}
}

@inproceedings{gravesSpeechRecognitionDeep2013,
  title = {Speech Recognition with Deep Recurrent Neural Networks},
  abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7\% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
  booktitle = {2013 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  doi = {10.1109/ICASSP.2013.6638947},
  author = {Graves, A. and Mohamed, A. and Hinton, G.},
  month = may,
  year = {2013},
  keywords = {Training,Recurrent neural networks,Acoustics,connectionist temporal classification,deep neural networks,deep recurrent neural networks,end-to-end training methods,long short-term memory RNN architecture,Noise,recurrent neural networks,sequential data,speech recognition,Speech recognition,Vectors},
  pages = {6645-6649},
  file = {/home/mo/Zotero/storage/JLIIAZ9U/6638947.html}
}

@article{schusterBidirectionalRecurrentNeural1997,
  title = {Bidirectional Recurrent Neural Networks},
  volume = {45},
  issn = {1053-587X},
  abstract = {In the first part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classification experiments on artificial data, the proposed structure gives better results than other approaches. For real data, classification experiments for phonemes from the TIMIT database show the same tendency. In the second part of this paper, it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution. For this part, experiments on real data are reported.},
  number = {11},
  journal = {IEEE Transactions on Signal Processing},
  doi = {10.1109/78.650093},
  author = {Schuster, M. and Paliwal, K. K.},
  month = nov,
  year = {1997},
  keywords = {recurrent neural nets,Recurrent neural networks,speech recognition,Speech recognition,artificial data,Artificial neural networks,bidirectional recurrent neural networks,classification experiments,complete symbol sequences,conditional posterior probability,Control systems,Databases,learning by example,learning from examples,negative time direction,Parameter estimation,pattern classification,phonemes,positive time direction,Probability,real data,regression experiments,regular recurrent neural network,Shape,speech processing,statistical analysis,Telecommunication control,TIMIT database,training,Training data},
  pages = {2673-2681},
  file = {/home/mo/Zotero/storage/SF3M24CM/Schuster and Paliwal - 1997 - Bidirectional recurrent neural networks.pdf;/home/mo/Zotero/storage/3KSZZL55/650093.html}
}

@inproceedings{gravesSpeechRecognitionDeep2013a,
  address = {{Vancouver, BC, Canada}},
  title = {Speech Recognition with Deep Recurrent Neural Networks},
  isbn = {978-1-4799-0356-6},
  abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7\% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
  language = {en},
  booktitle = {2013 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  publisher = {{IEEE}},
  doi = {10.1109/ICASSP.2013.6638947},
  author = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
  month = may,
  year = {2013},
  pages = {6645-6649},
  file = {/home/mo/Zotero/storage/3YC382Z8/Graves et al. - 2013 - Speech recognition with deep recurrent neural netw.pdf}
}

@article{waibelPhonemeRecognitionUsing1989,
  title = {Phoneme Recognition Using Time-Delay Neural Networks},
  volume = {37},
  issn = {0096-3518},
  abstract = {The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: (1) using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation; and (2) the time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independently of position in time and therefore not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes B, D, and G in varying phonetic contexts was chosen. For comparison, several discrete hidden Markov models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5\% correct while the rate obtained by the best of the HMMs was only 93.7\%.{$<>$}},
  number = {3},
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  doi = {10.1109/29.21701},
  author = {Waibel, A. and Hanazawa, T. and Hinton, G. and Shikano, K. and Lang, K. J.},
  month = mar,
  year = {1989},
  keywords = {Computer networks,Neural networks,speech recognition,Speech recognition,Acoustic testing,Backpropagation,Character recognition,Computer science,computing units,error backpropagation,hidden Markov models,Hidden Markov models,Loudspeakers,neural nets,nonlinear decision surfaces,phoneme recognition,Psychology,speech,temporal shifts,testing tokens,three-layer,time-delay neural networks},
  pages = {328-339},
  file = {/home/mo/Zotero/storage/6IJSTD2L/Waibel et al. - 1989 - Phoneme recognition using time-delay neural networ.pdf;/home/mo/Zotero/storage/DH32L682/21701.html}
}

@article{waibelPhonemeRecognitionUsing1989a,
  title = {Phoneme Recognition Using Time-Delay Neural Networks},
  volume = {37},
  abstract = {The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: (1) using a three-layer arrangement of simple computing units, a
hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation; and (2) the time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal
relationships between them independently of position in time and
therefore not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes B, D, and G in varying phonetic contexts was chosen. For comparison, several discrete hidden Markov models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5\% correct while the rate obtained by the best of the HMMs was only 93.7\%},
  journal = {Acoustics, Speech and Signal Processing, IEEE Transactions on},
  doi = {10.1109/29.21701},
  author = {Waibel, Alexander and Hanazawa, Toshiyuki and E. Hinton, G and Shikano, Kiyohiro and Lang, K.J.},
  month = apr,
  year = {1989},
  pages = {328-339},
  file = {/home/mo/Zotero/storage/F4JFXCQ7/Waibel et al. - 1989 - Phoneme recognition using time-delay neural networ.pdf}
}

@inproceedings{guoBackPropagationTime2013,
  title = {{{BackPropagation Through Time}}},
  abstract = {This report provides detailed description and necessary derivations for the BackPropagation Through Time (BPTT) algorithm. BPTT is often used to learn recurrent neural networks (RNN). Contrary to feed-forward neural networks, the RNN is characterized by the ability of encoding longer past information, thus very suitable for sequential models. The BPTT extends the ordinary BP algorithm to suit the recurrent neural architecture. 1 Basic Definitions For a two-layer feed-forward neural network, we notate the input layer as x indexed by variable i, the hidden layer as s indexed by variable j, and the output layer as y indexed by variable k. The weight matrix that map the input vector to the hidden layer is V, while the hidden layer is propagated through the weight matrix W, to the output layer. In a simple recurrent neural network, we attach every neural layer a time subscript t. The input layer consists of two components, x(t) and the privious activation of the hidden layer s(t - 1) indexed by variable h. The corresponding weight matrix is U. Table 1 lists all the notations used in this report: Neural layer Description Index variable x(t) input layer i s(t- 1) previous hidden (state) layer h s(t) hidden (state) layer j y(t) output layer k Weight matrix Description Index variables V Input layer \textrightarrow Hidden layer i, j U Previous hidden layer \textrightarrow Hidden layer h, j W Hidden layer \textrightarrow Output layer j, k Table 1: Notations in the recurrent neural network. Then, the recurrent neural network can be processed as the following: \textbullet{} Input layer \textrightarrow Hidden layer sj(t) = f(netj(t)) (1)},
  author = {Guo, Jiang},
  year = {2013},
  keywords = {Algorithm,Artificial neural network,Backpropagation through time,Biological Neural Networks,Feedforward neural network,Index,Multilayer perceptron,Neural Network Simulation,Random neural network,Recurrent neural network},
  file = {/home/mo/Zotero/storage/3LI8AUYW/Guo - 2013 - BackPropagation Through Time.pdf}
}

@article{werbosBackpropagationTimeWhat1990,
  title = {Backpropagation through Time: What It Does and How to Do It},
  volume = {78},
  issn = {0018-9219},
  shorttitle = {Backpropagation through Time},
  abstract = {Basic backpropagation, which is a simple method now being widely used in areas like pattern recognition and fault diagnosis, is reviewed. The basic equations for backpropagation through time, and applications to areas like pattern recognition involving dynamic systems, systems identification, and control are discussed. Further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations, or true recurrent networks, and other practical issues arising with the method are described. Pseudocode is provided to clarify the algorithms. The chain rule for ordered derivatives-the theorem which underlies backpropagation-is briefly discussed. The focus is on designing a simpler version of backpropagation which can be translated into computer code and applied directly by neutral network users.{$<>$}},
  number = {10},
  journal = {Proceedings of the IEEE},
  doi = {10.1109/5.58337},
  author = {Werbos, P. J.},
  month = oct,
  year = {1990},
  keywords = {Neural networks,Artificial neural networks,Control systems,Backpropagation,neural nets,backpropagation,Books,Equations,fault diagnosis,Fluid dynamics,identification,neural networks,pattern recognition,Pattern recognition,Power system modeling,pseudocode,Supervised learning,systems identification},
  pages = {1550-1560},
  file = {/home/mo/Zotero/storage/9QLT83YW/Werbos - 1990 - Backpropagation through time what it does and how.pdf;/home/mo/Zotero/storage/PXGDVCDM/58337.html}
}

@article{nagendraComparisonReinforcementLearning2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.01940},
  title = {Comparison of {{Reinforcement Learning}} Algorithms Applied to the {{Cart Pole}} Problem},
  abstract = {Designing optimal controllers continues to be challenging as systems are becoming complex and are inherently nonlinear. The principal advantage of reinforcement learning (RL) is its ability to learn from the interaction with the environment and provide optimal control strategy. In this paper, RL is explored in the context of control of the benchmark cartpole dynamical system with no prior knowledge of the dynamics. RL algorithms such as temporal-difference, policy gradient actor-critic, and value function approximation are compared in this context with the standard LQR solution. Further, we propose a novel approach to integrate RL and swing-up controllers.},
  journal = {2017 International Conference on Advances in Computing, Communications and Informatics (ICACCI)},
  doi = {10.1109/ICACCI.2017.8125811},
  author = {Nagendra, Savinay and Podila, Nikhil and Ugarakhod, Rashmi and George, Koshy},
  month = sep,
  year = {2017},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  pages = {26-32},
  file = {/home/mo/Zotero/storage/WYY4UGUZ/Nagendra et al. - 2017 - Comparison of Reinforcement Learning algorithms ap.pdf;/home/mo/Zotero/storage/ZF9A6Q49/1810.html}
}

@misc{TrajectoryGenerationUsing,
  title = {Trajectory Generation Using Reinforcement Learning for Autonomous Helicopter with Adaptive Dynamic Movement Primitive - {{Xiao Guo}}, 2017},
  howpublished = {https://journals.sagepub.com/doi/abs/10.1177/0959651816684427},
  file = {/home/mo/Zotero/storage/TV2AHWCN/0959651816684427.html}
}

@article{guoTrajectoryGenerationUsing2017,
  title = {Trajectory Generation Using Reinforcement Learning for Autonomous Helicopter with Adaptive Dynamic Movement Primitive},
  volume = {231},
  abstract = {The present paper introduces a smart trajectories generation algorithm for unmanned aerial vehicles under various environments. Dynamic movement primitive is extended by adding jerk to mock the kinematics, particularly for unmanned aerial vehicles. Combining the improved dynamic movement primitive with policy learning by weighted exploration with the returns, we propose the new algorithm producing optimal trajectories under different scenarios. Furthermore, numerical simulations under several scenarios are performed, demonstrating the ability of the proposed algorithm.},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part I: Journal of Systems and Control Engineering},
  doi = {10.1177/0959651816684427},
  author = {Guo, Xiao},
  month = may,
  year = {2017},
  pages = {095965181668442},
  file = {/home/mo/Zotero/storage/PAAIFL35/Guo - 2017 - Trajectory generation using reinforcement learning.pdf}
}

@article{neelakantanNeuralProgrammerInducing2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.04834},
  primaryClass = {cs, stat},
  title = {Neural {{Programmer}}: {{Inducing Latent Programs}} with {{Gradient Descent}}},
  shorttitle = {Neural {{Programmer}}},
  abstract = {Deep neural networks have achieved impressive supervised classification performance in many tasks including image recognition, speech recognition, and sequence to sequence learning. However, this success has not been translated to applications like question answering that may involve complex arithmetic and logic reasoning. A major limitation of these models is in their inability to learn even simple arithmetic and logic operations. For example, it has been shown that neural networks fail to learn to add two binary numbers reliably. In this work, we propose Neural Programmer, a neural network augmented with a small set of basic arithmetic and logic operations that can be trained end-to-end using backpropagation. Neural Programmer can call these augmented operations over several steps, thereby inducing compositional programs that are more complex than the built-in operations. The model learns from a weak supervision signal which is the result of execution of the correct program, hence it does not require expensive annotation of the correct program itself. The decisions of what operations to call, and what data segments to apply to are inferred by Neural Programmer. Such decisions, during training, are done in a differentiable fashion so that the entire network can be trained jointly by gradient descent. We find that training the model is difficult, but it can be greatly improved by adding random noise to the gradient. On a fairly complex synthetic table-comprehension dataset, traditional recurrent networks and attentional models perform poorly while Neural Programmer typically obtains nearly perfect accuracy.},
  language = {en},
  journal = {arXiv:1511.04834 [cs, stat]},
  author = {Neelakantan, Arvind and Le, Quoc V. and Sutskever, Ilya},
  month = nov,
  year = {2015},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Computation and Language},
  file = {/home/mo/Zotero/storage/QJKX3EUI/Neelakantan et al. - 2015 - Neural Programmer Inducing Latent Programs with G},
  annote = {Comment: Accepted as a conference paper at ICLR 2015}
}

@article{greffLSTMSearchSpace2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1503.04069},
  primaryClass = {cs},
  title = {{{LSTM}}: {{A Search Space Odyssey}}},
  shorttitle = {{{LSTM}}},
  abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs ({$\approx$} 15 years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
  language = {en},
  journal = {arXiv:1503.04069 [cs]},
  doi = {10.1109/TNNLS.2016.2582924},
  author = {Greff, Klaus and Srivastava, Rupesh Kumar and Koutn{\'i}k, Jan and Steunebrink, Bas R. and Schmidhuber, J{\"u}rgen},
  month = mar,
  year = {2015},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,68T10,H.5.5,I.2.6,I.2.7,I.5.1},
  file = {/home/mo/Zotero/storage/PINZXN2I/Greff et al. - 2015 - LSTM A Search Space Odyssey.pdf},
  annote = {Comment: 12 pages, 6 figures}
}

@misc{Cs234projectreportPdf,
  title = {Cs234-Project-Report.Pdf},
  journal = {Google Docs},
  howpublished = {https://drive.google.com/file/u/0/d/1tAYV33mPqJORuvL7Q8qBDbEraxkKm\_bC/view?usp=embed\_facebook},
  file = {/home/mo/Zotero/storage/T7FGHLEM/view.html}
}

@article{shangUniversityAmsterdamFaculty,
  title = {University of {{Amsterdam Faculty}} of {{Science Science Park}} 904 1098 {{XH Amsterdam}}},
  language = {en},
  author = {Shang, Wenling},
  pages = {31},
  file = {/home/mo/Zotero/storage/35LZTDGM/Shang - University of Amsterdam Faculty of Science Science.pdf}
}

@article{bohteSpikePropBackpropagationNetworks,
  title = {{{SpikeProp}}: {{Backpropagation}} for {{Networks}} of {{Spiking Neurons}}},
  abstract = {For a network of spiking neurons with reasonable postsynaptic potentials, we derive a supervised learning rule akin to traditional error-back-propagation, SpikeProp and show how to overcome the discontinuities introduced by thresholding. Using this learning algorithm, we demonstrate how networks of spiking neurons with biologically plausible time-constants can perform complex non-linear classification in fast temporal coding just as well as rate-coded networks. When comparing the (implicit) number of neurons required for the respective encodings, it is empirically demonstrated that temporal coding potentially requires significantly less neurons.},
  language = {en},
  author = {Bohte, Sander M and Kok, Joost N},
  pages = {7},
  file = {/home/mo/Zotero/storage/5VY7576I/Bohte and Kok - SpikeProp Backpropagation for Networks of Spiking.pdf}
}

@article{caporaleSpikeTimingDependent2008,
  title = {Spike {{Timing}}\textendash{{Dependent Plasticity}}: {{A Hebbian Learning Rule}}},
  volume = {31},
  shorttitle = {Spike {{Timing}}\textendash{{Dependent Plasticity}}},
  abstract = {Spike timing\textendash{}dependent plasticity (STDP) as a Hebbian synaptic learning rule has been demonstrated in various neural circuits over a wide spectrum of species, from insects to humans. The dependence of synaptic modification on the order of pre- and postsynaptic spiking within a critical window of tens of milliseconds has profound functional implications. Over the past decade, significant progress has been made in understanding the cellular mechanisms of STDP at both excitatory and inhibitory synapses and of the associated changes in neuronal excitability and synaptic integration. Beyond the basic asymmetric window, recent studies have also revealed several layers of complexity in STDP, including its dependence on dendritic location, the nonlinear integration of synaptic modification induced by complex spike trains, and the modulation of STDP by inhibitory and neuromodulatory inputs. Finally, the functional consequences of STDP have been examined directly in an increasing number of neural circuits in vivo.},
  number = {1},
  journal = {Annual Review of Neuroscience},
  doi = {10.1146/annurev.neuro.31.060407.125639},
  author = {Caporale, Natalia and Dan, Yang},
  year = {2008},
  pages = {25-46},
  pmid = {18275283}
}

@article{liActivityDependentSynapticPlasticity2014,
  title = {Activity-{{Dependent Synaptic Plasticity}} of a {{Chalcogenide Electronic Synapse}} for {{Neuromorphic Systems}}},
  volume = {4},
  abstract = {Nanoscale inorganic electronic synapses or synaptic devices, which are capable of emulating the functions of biological synapses of brain neuronal systems, are regarded as the basic building blocks for beyond-Von Neumann computing architecture, combining information storage and processing. Here, we demonstrate a Ag/AgInSbTe/Ag structure for chalcogenide memristor-based electronic synapses. The memristive characteristics with reproducible gradual resistance tuning are utilised to mimic the activity-dependent synaptic plasticity that serves as the basis of memory and learning. Bidirectional long-term Hebbian plasticity modulation is implemented by the coactivity of pre- and postsynaptic spikes, and the sign and degree are affected by assorted factors including the temporal difference, spike rate and voltage. Moreover, synaptic saturation is observed to be an adjustment of Hebbian rules to stabilise the growth of synaptic weights. Our results may contribute to the development of highly functional plastic electronic synapses and the further construction of next-generation parallel neuromorphic computing architecture.},
  journal = {Scientific reports},
  doi = {10.1038/srep04906},
  author = {Li, Yi and Zhong, Yingpeng and Zhang, Jinjian and {xu}, Lei and Wang, Qing and Sun, H.-J and Tong, Hao and Cheng, Xiaoming and Miao, X. S.},
  month = may,
  year = {2014},
  pages = {4906},
  file = {/home/mo/Zotero/storage/ELDPNKJN/Li et al. - 2014 - Activity-Dependent Synaptic Plasticity of a Chalco.pdf}
}

@misc{RandomSynapticFeedback,
  title = {Random Synaptic Feedback Weights Support Error Backpropagation for Deep Learning | {{Nature Communications}}},
  howpublished = {https://www.nature.com/articles/ncomms13276}
}

@misc{RandomSynapticFeedbacka,
  title = {Random Synaptic Feedback Weights Support Error Backpropagation for Deep Learning | {{Nature Communications}}},
  howpublished = {https://www.nature.com/articles/ncomms13276}
}

@article{lillicrapRandomFeedbackWeights2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1411.0247},
  primaryClass = {cs, q-bio},
  title = {Random Feedback Weights Support Learning in Deep Neural Networks},
  abstract = {The brain processes information through many layers of neurons. This deep architecture is representationally powerful, but it complicates learning by making it hard to identify the responsible neurons when a mistake is made. In machine learning, the backpropagation algorithm assigns blame to a neuron by computing exactly how it contributed to an error. To do this, it multiplies error signals by matrices consisting of all the synaptic weights on the neuron's axon and farther downstream. This operation requires a precisely choreographed transport of synaptic weight information, which is thought to be impossible in the brain. Here we present a surprisingly simple algorithm for deep learning, which assigns blame by multiplying error signals by random synaptic weights. We show that a network can learn to extract useful information from signals sent through these random feedback connections. In essence, the network learns to learn. We demonstrate that this new mechanism performs as quickly and accurately as backpropagation on a variety of problems and describe the principles which underlie its function. Our demonstration provides a plausible basis for how a neuron can be adapted using error signals generated at distal locations in the brain, and thus dispels long-held assumptions about the algorithmic constraints on learning in neural circuits.},
  journal = {arXiv:1411.0247 [cs, q-bio]},
  author = {Lillicrap, Timothy P. and Cownden, Daniel and Tweed, Douglas B. and Akerman, Colin J.},
  month = nov,
  year = {2014},
  keywords = {Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {/home/mo/Zotero/storage/VMDNPYA8/Lillicrap et al. - 2014 - Random feedback weights support learning in deep n.pdf;/home/mo/Zotero/storage/RY6DV6QR/1411.html},
  annote = {Comment: 14 pages, 5 figures in main text; 13 pages appendix}
}

@article{neftciSurrogateGradientLearning2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1901.09948},
  primaryClass = {cs, q-bio},
  title = {Surrogate {{Gradient Learning}} in {{Spiking Neural Networks}}},
  abstract = {Spiking neural networks are nature's versatile solution to fault-tolerant and energy efficient signal processing. To translate these benefits into hardware, a growing number of neuromorphic spiking neural network processors attempt to emulate biological neural networks. These developments have created an imminent need for methods and tools to enable such systems to solve real-world signal processing problems. Like conventional neural networks, spiking neural networks can be trained on real, domain specific data. However, their training requires overcoming a number of challenges linked to their binary and dynamical nature. This article elucidates step-by-step the problems typically encountered when training spiking neural networks, and guides the reader through the key concepts of synaptic plasticity and data-driven learning in the spiking setting. To that end, it gives an overview of existing approaches and provides an introduction to surrogate gradient methods, specifically, as a particularly flexible and efficient method to overcome the aforementioned challenges.},
  journal = {arXiv:1901.09948 [cs, q-bio]},
  author = {Neftci, Emre O. and Mostafa, Hesham and Zenke, Friedemann},
  month = jan,
  year = {2019},
  keywords = {Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {/home/mo/Zotero/storage/UIB5H7IQ/Neftci et al. - 2019 - Surrogate Gradient Learning in Spiking Neural Netw.pdf;/home/mo/Zotero/storage/3I8JB2X8/1901.html}
}

@misc{SupervisedLearningSpiking,
  title = {Supervised {{Learning}} in {{Spiking Neural Networks}} with {{ReSuMe}}: {{Sequence Learning}}, {{Classification}}, and {{Spike Shifting}} | {{Neural Computation}} | {{MIT Press Journals}}},
  howpublished = {https://www.mitpressjournals.org/doi/abs/10.1162/neco.2009.11-08-901}
}

@incollection{bellecLongShorttermMemory2018a,
  title = {Long Short-Term Memory and {{Learning}}-to-Learn in Networks of Spiking Neurons},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  publisher = {{Curran Associates, Inc.}},
  author = {Bellec, Guillaume and Salaj, Darjan and Subramoney, Anand and Legenstein, Robert and Maass, Wolfgang},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {787--797},
  file = {/home/mo/Zotero/storage/D7TDKE6P/Bellec et al. - 2018 - Long short-term memory and Learning-to-learn in ne.pdf;/home/mo/Zotero/storage/Y8BTXKFQ/7359-long-short-term-memory-and-learning-to-learn-in-networks-of-spiking-neurons.html}
}

@inproceedings{mahadevuniNavigatingMobileRobots2017,
  address = {{Anchorage, AK, USA}},
  title = {Navigating Mobile Robots to Target in near Shortest Time Using Reinforcement Learning with Spiking Neural Networks},
  isbn = {978-1-5090-6182-2},
  abstract = {The autonomous navigation of mobile robots in unknown environments is of great interest in mobile robotics. This article discusses a new strategy to navigate to a known target location in an unknown environment using a combination of the ``go-to-goal'' approach and reinforcement learning with biologically realistic spiking neural networks. While the ``goto-goal'' approach itself might lead to a solution for most environments, the added neural reinforcement learning in this work results in a strategy that takes the robot from a starting position to a target location in a near shortest possible time. To achieve the goal, we propose a reinforcement learning approach based on spiking neural networks. The presented biologically motivated delayed reward mechanism using eligibility traces results in a greedy approach that leads the robot to the target in a close to shortest possible time.},
  language = {en},
  booktitle = {2017 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  publisher = {{IEEE}},
  doi = {10.1109/IJCNN.2017.7966127},
  author = {Mahadevuni, Amarnath and Li, Peng},
  month = may,
  year = {2017},
  pages = {2243-2250},
  file = {/home/mo/Zotero/storage/L2SMXR33/Mahadevuni and Li - 2017 - Navigating mobile robots to target in near shortes.pdf}
}

@article{herrmannTranscranialAlternatingCurrent2013,
  title = {Transcranial Alternating Current Stimulation: {{A}} Review of the Underlying Mechanisms and Modulation of Cognitive Processes},
  volume = {7},
  shorttitle = {Transcranial Alternating Current Stimulation},
  abstract = {Brain oscillations of different frequencies have been associated with a variety of cognitive functions. Convincing evidence supporting those associations has been provided by studies using intracranial stimulation, pharmacological interventions and lesion studies. The emergence of novel non-invasive brain stimulation techniques like repetitive transcranial magnetic stimulation (rTMS) and transcranial alternating current stimulation (tACS) now allows to modulate brain oscillations directly. Particularly, tACS offers the unique opportunity to causally link brain oscillations of a specific frequency range to cognitive processes, because it uses sinusoidal currents that are bound to one frequency only. Using tACS allows to modulate brain oscillations and in turn to influence cognitive processes, thereby demonstrating the causal link between the two. Here, we review findings about the physiological mechanism of tACS and studies that have used tACS to modulate basic motor and sensory processes as well as higher cognitive processes like memory, ambiguous perception, and decision making.},
  journal = {Frontiers in human neuroscience},
  doi = {10.3389/fnhum.2013.00279},
  author = {Herrmann, Christoph and Rach, Stefan and Neuling, Toralf and Str{\"u}ber, Daniel},
  month = jun,
  year = {2013},
  pages = {279},
  file = {/home/mo/Zotero/storage/Y5VS6NAB/Herrmann et al. - 2013 - Transcranial alternating current stimulation A re.pdf}
}

@misc{NetworkSimulationTACS,
  title = {Network Simulation of {{tACS}}. ({{A}}) {{Spike}} Timing Dependent Plasticity:...},
  shorttitle = {Network Simulation of {{tACS}}. ({{A}}) {{Spike}} Timing Dependent Plasticity},
  abstract = {Download scientific diagram | Network simulation of tACS. (A) Spike timing dependent plasticity: synaptic weights are increased if a post-synaptic potential follows a pre-synaptic spike (long-term potentiation, LTP) and decreased if a post-synaptic potential occurs prior to a pre-synaptic spike (long-term depression, LTD). (B) Schematic illustration of the network: A driving neuron establishes a recurrent loop with each neuron of a hidden layer. The total synaptic delay, {$\Delta$}t, (i.e., the sum of both delays of the loop) varied between 20 and 160 ms. The driving neuron was stimulated with a spike train of 10 Hz repetition rate. (C) Synaptic weights of the back-projection as a function of the total synaptic delay of the recurrent loops: Gray dots display synaptic weights at the start of the simulation, black dots represent synaptic weights after the end of simulation. External stimulation of the driving neuron at 10 Hz resulted in increased weights for recurrent loops with a total delay between 60 and 100 ms, and dramatically reduced synaptic weights for loops with total delays outside this interval. Note, that the highest synaptic weights are observed at 100 ms, i.e., for loops with a resonance frequency near the stimulation frequency. Reprinted from Zaehle et al. (2010) with permission of the authors. from publication: Transcranial alternating current stimulation: A review of the underlying mechanisms and modulation of cognitive processes | Brain oscillations of different frequencies have been associated with a variety of cognitive functions. Convincing evidence supporting those associations has been provided by studies using intracranial stimulation, pharmacological interventions and lesion studies. The... | Stimulation, Cognitive and Transcranial direct current stimulation | ResearchGate, the professional network for scientists.},
  language = {en},
  journal = {ResearchGate},
  howpublished = {https://www.researchgate.net/figure/Network-simulation-of-tACS-A-Spike-timing-dependent-plasticity-synaptic-weights-are\_fig6\_240306946},
  file = {/home/mo/Zotero/storage/XF8YDI5N/Network-simulation-of-tACS-A-Spike-timing-dependent-plasticity-synaptic-weights-are_fig6_240306.html}
}

@article{wangPrefrontalCortexMetareinforcement2018a,
  title = {Prefrontal Cortex as a Meta-Reinforcement Learning System},
  volume = {21},
  copyright = {2018 The Author(s)},
  issn = {1546-1726},
  abstract = {Humans and other mammals are prodigious learners, partly because they also `learn how to learn'. Wang and colleagues present a new theory showing how learning to learn may arise from interactions between prefrontal cortex and the dopamine system.},
  language = {En},
  number = {6},
  journal = {Nature Neuroscience},
  doi = {10.1038/s41593-018-0147-8},
  author = {Wang, Jane X. and {Kurth-Nelson}, Zeb and Kumaran, Dharshan and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z. and Hassabis, Demis and Botvinick, Matthew},
  month = jun,
  year = {2018},
  pages = {860},
  file = {/home/mo/Zotero/storage/MM3UEJNQ/s41593-018-0147-8.html}
}

@article{bohnstinglNeuromorphicHardwareLearns2019,
  title = {Neuromorphic {{Hardware Learns}} to {{Learn}}},
  volume = {13},
  issn = {1662-4548},
  abstract = {Hyperparameters and learning algorithms for neuromorphic hardware are usually chosen by hand to suit a particular task. In contrast, networks of neurons in the brain were optimized through extensive evolutionary and developmental processes to work well on a range of computing and learning tasks. Occasionally this process has been emulated through genetic algorithms, but these require themselves hand-design of their details and tend to provide a limited range of improvements. We employ instead other powerful gradient-free optimization tools, such as cross-entropy methods and evolutionary strategies, in order to port the function of biological optimization processes to neuromorphic hardware. As an example, we show these optimization algorithms enable neuromorphic agents to learn very efficiently from rewards. In particular, meta-plasticity, i.e., the optimization of the learning rule which they use, substantially enhances reward-based learning capability of the hardware. In addition, we demonstrate for the first time Learning-to-Learn benefits from such hardware, in particular, the capability to extract abstract knowledge from prior learning experiences that speeds up the learning of new but related tasks. Learning-to-Learn is especially suited for accelerated neuromorphic hardware, since it makes it feasible to carry out the required very large number of network computations.},
  journal = {Frontiers in Neuroscience},
  doi = {10.3389/fnins.2019.00483},
  author = {Bohnstingl, Thomas and Scherr, Franz and Pehle, Christian and Meier, Karlheinz and Maass, Wolfgang},
  month = may,
  year = {2019},
  file = {/home/mo/Zotero/storage/LBVB4MDW/Bohnstingl et al. - 2019 - Neuromorphic Hardware Learns to Learn.pdf},
  pmid = {31178681},
  pmcid = {PMC6536858}
}

@article{zadorCritiquePureLearning2019,
  title = {A {{Critique}} of {{Pure Learning}}: {{What Artificial Neural Networks}} Can {{Learn}} from {{Animal Brains}}},
  copyright = {\textcopyright{} 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  shorttitle = {A {{Critique}} of {{Pure Learning}}},
  abstract = {{$<$}h3{$>$}ABSTRACT{$<$}/h3{$>$} {$<$}p{$>$}Over the last decade, artificial neural networks (ANNs), have undergone a revolution, catalyzed in large part by better tools for supervised learning. However, training such networks requires enormous data sets of labeled examples, whereas young animals (including humans) typically learn with few or no labeled examples. This stark contrast with biological learning has led many in the ANN community posit that instead of supervised paradigms, animals must rely instead primarily on unsupervised learning, leading the search for better unsupervised algorithms. Here we argue that much of an animal's behavioral repertoire is not the result of clever learning algorithms\textemdash{}supervised or unsupervised\textemdash{}but arises instead from behavior programs already present at birth. These programs arise through evolution, are encoded in the genome, and emerge as a consequence of wiring up the brain. Specifically, animals are born with highly structured brain connectivity, which enables them learn very rapidly. Recognizing the importance of the highly structured connectivity suggests a path toward building ANNs capable of rapid learning.{$<$}/p{$>$}},
  language = {en},
  journal = {bioRxiv},
  doi = {10.1101/582643},
  author = {Zador, Anthony M.},
  month = mar,
  year = {2019},
  pages = {582643},
  file = {/home/mo/Zotero/storage/WKK5FVKU/Zador - 2019 - A Critique of Pure Learning What Artificial Neura.pdf;/home/mo/Zotero/storage/J4QGA3SW/582643v1.html}
}

@article{maassNetworksSpikingNeurons,
  title = {Networks of {{Spiking Neurons Learn}} to {{Learn}} and {{Remember}}},
  language = {en},
  author = {Maass, Wolfgang},
  pages = {27},
  file = {/home/mo/Zotero/storage/5GD973Z9/Maass - Networks of Spiking Neurons Learn to Learn and Rem.pdf}
}

@article{bellecBiologicallyInspiredAlternatives2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1901.09049},
  primaryClass = {cs},
  title = {Biologically Inspired Alternatives to Backpropagation through Time for Learning in Recurrent Neural Nets},
  abstract = {The way how recurrently connected networks of spiking neurons in the brain acquire powerful information processing capabilities through learning has remained a mystery. This lack of understanding is linked to a lack of learning algorithms for recurrent networks of spiking neurons (RSNNs) that are both functionally powerful and can be implemented by known biological mechanisms. Since RSNNs are simultaneously a primary target for implementations of brain-inspired circuits in neuromorphic hardware, this lack of algorithmic insight also hinders technological progress in that area. The gold standard for learning in recurrent neural networks in machine learning is back-propagation through time (BPTT), which implements stochastic gradient descent with regard to a given loss function. But BPTT is unrealistic from a biological perspective, since it requires a transmission of error signals backwards in time and in space, i.e., from post- to presynaptic neurons. We show that an online merging of locally available information during a computation with suitable top-down learning signals in real-time provides highly capable approximations to BPTT. For tasks where information on errors arises only late during a network computation, we enrich locally available information through feedforward eligibility traces of synapses that can easily be computed in an online manner. The resulting new generation of learning algorithms for recurrent neural networks provides a new understanding of network learning in the brain that can be tested experimentally. In addition, these algorithms provide efficient methods for on-chip training of RSNNs in neuromorphic hardware.},
  language = {en},
  journal = {arXiv:1901.09049 [cs]},
  author = {Bellec, Guillaume and Scherr, Franz and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
  month = jan,
  year = {2019},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/home/mo/Zotero/storage/78IJKMXM/Bellec_eProp_poster.pdf},
  annote = {Comment: We changed in this version 2 of the paper the name of the new learning algorithms to e-prop, corrected minor errors, added details -- especially for resulting new rules for synaptic plasticity, edited the notation, and included new results for TIMIT}
}

@article{lillicrapRandomSynapticFeedback2016,
  title = {Random Synaptic Feedback Weights Support Error Backpropagation for Deep Learning},
  volume = {7},
  copyright = {2016 Nature Publishing Group},
  issn = {2041-1723},
  abstract = {The brain processes information through multiple layers of neurons. This deep architecture is representationally powerful, but complicates learning because it is difficult to identify the responsible neurons when a mistake is made. In machine learning, the backpropagation algorithm assigns blame by multiplying error signals with all the synaptic weights on each neuron's axon and further downstream. However, this involves a precise, symmetric backward connectivity pattern, which is thought to be impossible in the brain. Here we demonstrate that this strong architectural constraint is not required for effective error propagation. We present a surprisingly simple mechanism that assigns blame by multiplying errors by even random synaptic weights. This mechanism can transmit teaching signals across multiple layers of neurons and performs as effectively as backpropagation on a variety of tasks. Our results help reopen questions about how the brain could use error signals and dispel long-held assumptions about algorithmic constraints on learning.},
  language = {en},
  journal = {Nature Communications},
  doi = {10.1038/ncomms13276},
  author = {Lillicrap, Timothy P. and Cownden, Daniel and Tweed, Douglas B. and Akerman, Colin J.},
  month = nov,
  year = {2016},
  pages = {13276},
  file = {/home/mo/Zotero/storage/39D3ZYGN/Lillicrap et al. - 2016 - Random synaptic feedback weights support error bac.pdf;/home/mo/Zotero/storage/WCUFHJKJ/ncomms13276.html}
}

@book{ponulakIntroductionSpikingNeural2011,
  title = {Introduction to Spiking Neural Networks: {{Information}} Processing, Learning and Applications},
  volume = {71},
  shorttitle = {Introduction to Spiking Neural Networks},
  abstract = {The concept that neural information is encoded in the firing rate of neurons has been the dominant paradigm in neurobiology for many years. This paradigm has also been adopted by the theory of artificial neural networks. Recent physiological experiments demonstrate, however, that in many parts of the nervous system, neural code is founded on the timing of individual action potentials. This finding has given rise to the emergence of a new class of neural models, called spiking neural networks. In this paper we summarize basic properties of spiking neurons and spiking networks. Our focus is, specifically, on models of spike-based information coding, synaptic plasticity and learning. We also survey real-life applications of spiking models. The paper is meant to be an introduction to spiking neural networks for scientists from various disciplines interested in spike-based neural processing.},
  author = {Ponulak, Filip and Kasi{\'n}ski, Andrzej},
  month = jan,
  year = {2011},
  file = {/home/mo/Zotero/storage/PT34XZ65/Ponulak and Kasiski - 2011 - Introduction to spiking neural networks Informati.pdf}
}

@inproceedings{bohteErrorBackpropagationNetworksFractionally2011,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Error-{{Backpropagation}} in {{Networks}} of {{Fractionally Predictive Spiking Neurons}}},
  isbn = {978-3-642-21735-7},
  abstract = {We develop a learning rule for networks of spiking neurons where signals are encoded using fractionally predictive spike-coding. In this paradigm, neural output signals are encoded as a sum of shifted power-law kernels. Simple greedy thresholding can compute this encoding, and spike-trains are then exactly the signal's fractional derivative. Fractionally predictive spike-coding exploits natural statistics and is consistent with observed spike-rate adaptation in real neurons; its multiple-timescale properties also reconciles notions of spike-time coding and spike-rate coding. Previously, we argued that properly tuning the decoding kernel at receiving neurons can implement spectral filtering; the applicability to general temporal filtering was left open. Here, we present an error-backpropagation algorithm to learn these decoding filters, and we show that networks of fractionally predictive spiking neurons can then implement temporal filters such as delayed responses, delayed match-to-sampling, and temporal versions of the XOR problem.},
  language = {en},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} \textendash{} {{ICANN}} 2011},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Bohte, Sander M.},
  editor = {Honkela, Timo and Duch, W{\l}odzis{\l}aw and Girolami, Mark and Kaski, Samuel},
  year = {2011},
  keywords = {Hide Neuron,Longe Timescale,Neural Computation,Output Neuron,Real Neuron},
  pages = {60-68},
  file = {/home/mo/Zotero/storage/QGQ39LUL/Bohte - 2011 - Error-Backpropagation in Networks of Fractionally .pdf}
}

@article{neftciSurrogateGradientLearning2019a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1901.09948},
  primaryClass = {cs, q-bio},
  title = {Surrogate {{Gradient Learning}} in {{Spiking Neural Networks}}},
  abstract = {Spiking neural networks are nature's versatile solution to fault-tolerant and energy efficient signal processing. To translate these benefits into hardware, a growing number of neuromorphic spiking neural network processors attempt to emulate biological neural networks. These developments have created an imminent need for methods and tools to enable such systems to solve real-world signal processing problems. Like conventional neural networks, spiking neural networks can be trained on real, domain specific data. However, their training requires overcoming a number of challenges linked to their binary and dynamical nature. This article elucidates step-by-step the problems typically encountered when training spiking neural networks, and guides the reader through the key concepts of synaptic plasticity and data-driven learning in the spiking setting. To that end, it gives an overview of existing approaches and provides an introduction to surrogate gradient methods, specifically, as a particularly flexible and efficient method to overcome the aforementioned challenges.},
  journal = {arXiv:1901.09948 [cs, q-bio]},
  author = {Neftci, Emre O. and Mostafa, Hesham and Zenke, Friedemann},
  month = jan,
  year = {2019},
  keywords = {Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {/home/mo/Zotero/storage/WVPFHJSL/Neftci et al. - 2019 - Surrogate Gradient Learning in Spiking Neural Netw.pdf;/home/mo/Zotero/storage/93B2FPVU/1901.html}
}

@misc{NetworkSimulationTACSa,
  title = {Network Simulation of {{tACS}}. ({{A}}) {{Spike}} Timing Dependent Plasticity:...},
  shorttitle = {Network Simulation of {{tACS}}. ({{A}}) {{Spike}} Timing Dependent Plasticity},
  abstract = {Download scientific diagram | Network simulation of tACS. (A) Spike timing dependent plasticity: synaptic weights are increased if a post-synaptic potential follows a pre-synaptic spike (long-term potentiation, LTP) and decreased if a post-synaptic potential occurs prior to a pre-synaptic spike (long-term depression, LTD). (B) Schematic illustration of the network: A driving neuron establishes a recurrent loop with each neuron of a hidden layer. The total synaptic delay, {$\Delta$}t, (i.e., the sum of both delays of the loop) varied between 20 and 160 ms. The driving neuron was stimulated with a spike train of 10 Hz repetition rate. (C) Synaptic weights of the back-projection as a function of the total synaptic delay of the recurrent loops: Gray dots display synaptic weights at the start of the simulation, black dots represent synaptic weights after the end of simulation. External stimulation of the driving neuron at 10 Hz resulted in increased weights for recurrent loops with a total delay between 60 and 100 ms, and dramatically reduced synaptic weights for loops with total delays outside this interval. Note, that the highest synaptic weights are observed at 100 ms, i.e., for loops with a resonance frequency near the stimulation frequency. Reprinted from Zaehle et al. (2010) with permission of the authors. from publication: Transcranial alternating current stimulation: A review of the underlying mechanisms and modulation of cognitive processes | Brain oscillations of different frequencies have been associated with a variety of cognitive functions. Convincing evidence supporting those associations has been provided by studies using intracranial stimulation, pharmacological interventions and lesion studies. The... | Stimulation, Transcranial direct current stimulation and Transcranial Magnetic Stimulation | ResearchGate, the professional network for scientists.},
  language = {en},
  journal = {ResearchGate},
  howpublished = {https://www.researchgate.net/figure/Network-simulation-of-tACS-A-Spike-timing-dependent-plasticity-synaptic-weights-are\_fig6\_240306946},
  file = {/home/mo/Zotero/storage/97MIZATN/Network-simulation-of-tACS-A-Spike-timing-dependent-plasticity-synaptic-weights-are_fig6_240306.html}
}

@article{bellecDeepRewiringTraining2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.05136},
  primaryClass = {cs, stat},
  title = {Deep {{Rewiring}}: {{Training}} Very Sparse Deep Networks},
  shorttitle = {Deep {{Rewiring}}},
  abstract = {Neuromorphic hardware tends to pose limits on the connectivity of deep networks that one can run on them. But also generic hardware and software implementations of deep learning run more efficiently for sparse networks. Several methods exist for pruning connections of a neural network after it was trained without connectivity constraints. We present an algorithm, DEEP R, that enables us to train directly a sparsely connected neural network. DEEP R automatically rewires the network during supervised training so that connections are there where they are most needed for the task, while its total number is all the time strictly bounded. We demonstrate that DEEP R can be used to train very sparse feedforward and recurrent neural networks on standard benchmark tasks with just a minor loss in performance. DEEP R is based on a rigorous theoretical foundation that views rewiring as stochastic sampling of network configurations from a posterior.},
  journal = {arXiv:1711.05136 [cs, stat]},
  author = {Bellec, Guillaume and Kappel, David and Maass, Wolfgang and Legenstein, Robert},
  month = nov,
  year = {2017},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Distributed; Parallel; and Cluster Computing},
  file = {/home/mo/Zotero/storage/DYLGSNUZ/Bellec et al. - 2017 - Deep Rewiring Training very sparse deep networks.pdf;/home/mo/Zotero/storage/IDIUG3TC/1711.html},
  annote = {Comment: Accepted for publication at ICLR 2018. 10 pages (12 with references, 24 with appendix), 4 Figures in the main text. Reviews are available at: https://openreview.net/forum?id=BJ\_wN01C- . This recent version contains minor corrections in the appendix}
}

@article{panDopamineCellsRespond2005,
  title = {Dopamine {{Cells Respond}} to {{Predicted Events}} during {{Classical Conditioning}}: {{Evidence}} for {{Eligibility Traces}} in the {{Reward}}-{{Learning Network}}},
  volume = {25},
  copyright = {Copyright \textcopyright{} 2005 Society for Neuroscience 0270-6474/05/256235-08.00/0},
  issn = {0270-6474, 1529-2401},
  shorttitle = {Dopamine {{Cells Respond}} to {{Predicted Events}} during {{Classical Conditioning}}},
  abstract = {Behavioral conditioning of cue-reward pairing results in a shift of midbrain dopamine (DA) cell activity from responding to the reward to responding to the predictive cue. However, the precise time course and mechanism underlying this shift remain unclear. Here, we report a combined single-unit recording and temporal difference (TD) modeling approach to this question. The data from recordings in conscious rats showed that DA cells retain responses to predicted reward after responses to conditioned cues have developed, at least early in training. This contrasts with previous TD models that predict a gradual stepwise shift in latency with responses to rewards lost before responses develop to the conditioned cue. By exploring the TD parameter space, we demonstrate that the persistent reward responses of DA cells during conditioning are only accurately replicated by a TD model with long-lasting eligibility traces (nonzero values for the parameter {$\lambda$}) and low learning rate ({$\alpha$}). These physiological constraints for TD parameters suggest that eligibility traces and low per-trial rates of plastic modification may be essential features of neural circuits for reward learning in the brain. Such properties enable rapid but stable initiation of learning when the number of stimulus-reward pairings is limited, conferring significant adaptive advantages in real-world environments.},
  language = {en},
  number = {26},
  journal = {Journal of Neuroscience},
  doi = {10.1523/JNEUROSCI.1478-05.2005},
  author = {Pan, Wei-Xing and Schmidt, Robert and Wickens, Jeffery R. and Hyland, Brian I.},
  month = jun,
  year = {2005},
  keywords = {associative learning,dopaminergic,extracellular recordings,reward,temporal difference algorithm,ventral tegmental area},
  pages = {6235-6242},
  file = {/home/mo/Zotero/storage/TPQSZ9N8/Pan et al. - 2005 - Dopamine Cells Respond to Predicted Events during .pdf;/home/mo/Zotero/storage/TIF7W4P2/6235.html},
  pmid = {15987953}
}

@book{ponulakIntroductionSpikingNeural2011a,
  title = {Introduction to Spiking Neural Networks: {{Information}} Processing, Learning and Applications},
  volume = {71},
  shorttitle = {Introduction to Spiking Neural Networks},
  abstract = {The concept that neural information is encoded in the firing rate of neurons has been the dominant paradigm in neurobiology for many years. This paradigm has also been adopted by the theory of artificial neural networks. Recent physiological experiments demonstrate, however, that in many parts of the nervous system, neural code is founded on the timing of individual action potentials. This finding has given rise to the emergence of a new class of neural models, called spiking neural networks. In this paper we summarize basic properties of spiking neurons and spiking networks. Our focus is, specifically, on models of spike-based information coding, synaptic plasticity and learning. We also survey real-life applications of spiking models. The paper is meant to be an introduction to spiking neural networks for scientists from various disciplines interested in spike-based neural processing.},
  author = {Ponulak, Filip and Kasi{\'n}ski, Andrzej},
  month = jan,
  year = {2011},
  file = {/home/mo/Zotero/storage/T9DYK8GE/Ponulak and Kasiski - 2011 - Introduction to spiking neural networks Informati.pdf}
}

@article{legensteinLearningTheoryRewardModulated2008,
  title = {A {{Learning Theory}} for {{Reward}}-{{Modulated Spike}}-{{Timing}}-{{Dependent Plasticity}} with {{Application}} to {{Biofeedback}}},
  volume = {4},
  abstract = {Reward-modulated spike-timing-dependent plasticity (STDP) has recently emerged as a candidate for a learning rule that could explain how behaviorally relevant adaptive changes in complex networks of spiking neurons could be achieved in a self-organizing manner through local synaptic plasticity. However, the capabilities and limitations of this learning rule could so far only be tested through computer simulations. This article provides tools for an analytic treatment of reward-modulated STDP, which allows us to predict under which conditions reward-modulated STDP will achieve a desired learning effect. These analytical results imply that neurons can learn through reward-modulated STDP to classify not only spatial but also temporal firing patterns of presynaptic neurons. They also can learn to respond to specific presynaptic firing patterns with particular spike patterns. Finally, the resulting learning theory predicts that even difficult credit-assignment problems, where it is very hard to tell which synaptic weights should be modified in order to increase the global reward for the system, can be solved in a self-organizing manner through reward-modulated STDP. This yields an explanation for a fundamental experimental result on biofeedback in monkeys by Fetz and Baker. In this experiment monkeys were rewarded for increasing the firing rate of a particular neuron in the cortex and were able to solve this extremely difficult credit assignment problem. Our model for this experiment relies on a combination of reward-modulated STDP with variable spontaneous firing activity. Hence it also provides a possible functional explanation for trial-to-trial variability, which is characteristic for cortical networks of neurons but has no analogue in currently existing artificial computing systems. In addition our model demonstrates that reward-modulated STDP can be applied to all synapses in a large recurrent neural network without endangering the stability of the network dynamics.},
  journal = {PLoS computational biology},
  doi = {10.1371/journal.pcbi.1000180},
  author = {Legenstein, Robert and Pecevski, Dejan and Maass, Wolfgang},
  month = nov,
  year = {2008},
  pages = {e1000180},
  file = {/home/mo/Zotero/storage/VKN2R2WY/Legenstein et al. - 2008 - A Learning Theory for Reward-Modulated Spike-Timin.pdf}
}

@article{dikmanErrorMonitoringReward2000,
  title = {Error Monitoring during Reward and Avoidance Learning in High- and Low-Socialized Individuals},
  volume = {37},
  copyright = {Copyright \textcopyright{} 2003 Society for Psychophysiological Research},
  issn = {1469-8986},
  abstract = {The error-related negativity (ERN) is a response-locked brain potential generated when individuals make mistakes during simple decision-making tasks. In the present study, we examined ERN under conditions of reward and punishment, among participants who scored extremely low or high on the socialization scale of the California Psychological Inventory (CPI). Participants completed a forced-choice task, and were rewarded for correct responses in half the trials, and punished for incorrect responses in the remaining trials. A significant interaction between socialization (SO) and condition revealed that low-SO participants produced smaller ERNs during the punishment task than during the reward task, whereas high-SO participants produced similar ERNs in both conditions. Reaction time and electromyogram data essentially bolster the interpretation that the ERN effects reflect differences in error salience for high-SO and low-SO participants, and are consistent with the avoidance-learning deficits seen in psychopathy.},
  language = {en},
  number = {1},
  journal = {Psychophysiology},
  doi = {10.1111/1469-8986.3710043},
  author = {Dikman, Ziya V. and Allen, John J. B.},
  year = {2000},
  keywords = {Error-related negativity,Event-related brain potential (ERP),Punishment,Reward,Socialization},
  pages = {43-54},
  file = {/home/mo/Zotero/storage/YMLXWF5E/1469-8986.html}
}

@incollection{botevCrossEntropyMethodOptimization2013,
  title = {The {{Cross}}-{{Entropy Method}} for {{Optimization}}},
  volume = {31},
  isbn = {978-0-444-53859-8},
  abstract = {The cross-entropy method is a versatile heuristic tool for solving difficult estimation and optimization problems, based on Kullback\textendash{}Leibler (or cross-entropy) minimization. As an optimization method it unifies many existing populationbased optimization heuristics. In this chapter we show how the cross-entropy method can be applied to a diverse range of combinatorial, continuous, and noisy optimization problems.},
  language = {en},
  booktitle = {Handbook of {{Statistics}}},
  publisher = {{Elsevier}},
  author = {Botev, Zdravko I. and Kroese, Dirk P. and Rubinstein, Reuven Y. and L'Ecuyer, Pierre},
  year = {2013},
  pages = {35-59},
  file = {/home/mo/Zotero/storage/BS6PI7MN/Botev et al. - 2013 - The Cross-Entropy Method for Optimization.pdf},
  doi = {10.1016/B978-0-444-53859-8.00003-5}
}

@article{beyerEvolutionStrategiesComprehensive2002,
  title = {Evolution Strategies - {{A}} Comprehensive Introduction},
  volume = {1},
  abstract = {This article gives a comprehensive,introduction into one of the main branches of evolutionary computation,\textendash{} the evolution strategies (ES) the history of which dates back to the 1960s in Germany. Starting from a survey of history the philosophical background,is explained in order to make,understandable why,ES are realized in the way they are. Basic ES algorithms and design principles for variation and selection operators as well as theoretical issues are presented, and future branches of ES research are discussed. Key words: computational intelligence, Darwinian evolution, design principles for genetic},
  journal = {Natural Computing},
  doi = {10.1023/A:1015059928466},
  author = {Beyer, Hans-Georg and Schwefel, Hans-Paul},
  month = mar,
  year = {2002},
  pages = {3-52},
  file = {/home/mo/Zotero/storage/3RRGWR87/Beyer and Schwefel - 2002 - Evolution strategies - A comprehensive introductio.pdf}
}

@book{TheoryEvolutionStrategies2001,
  address = {{Berlin ; New York}},
  title = {The {{Theory}} of {{Evolution Strategies}}},
  isbn = {978-3-540-67297-5},
  year = {2001}
}

@article{kirkpatrickOptimizationSimulatedAnnealing1983,
  title = {Optimization by Simulated Annealing.},
  volume = {220},
  abstract = {There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods.},
  number = {4598},
  journal = {Science},
  doi = {10.1126/science.220.4598.671},
  author = {Kirkpatrick, Scott and Gelatt, Charles Daniel and Vecchi, Mario P.},
  year = {1983},
  keywords = {solid substance,Statistical Mechanics},
  pages = {671-680},
  file = {/home/mo/Zotero/storage/ASCHU9NK/Kirkpatrick et al. - 1983 - Optimization by simulated annealing..pdf}
}

@misc{ScalableDeepReinforcement,
  title = {Scalable {{Deep Reinforcement Learning}} for {{Robotic Manipulation}}},
  abstract = {Posted Alex Irpan, Software Engineer, Google Brain Team and Peter Pastor, Senior Roboticist, X   How can robots acquire skills that generali...},
  language = {en},
  journal = {Google AI Blog},
  file = {/home/mo/Zotero/storage/CX52F8QY/scalable-deep-reinforcement-learning.html}
}

@misc{LearningDexterity2018,
  title = {Learning {{Dexterity}}},
  abstract = {We've trained a human-like robot hand to manipulate physical objects with unprecedented dexterity.},
  language = {en},
  journal = {OpenAI},
  howpublished = {https://openai.com/blog/learning-dexterity/},
  month = jul,
  year = {2018},
  file = {/home/mo/Zotero/storage/SBSP8RFF/learning-dexterity.html}
}

@misc{FrameworkMesencephalicDopamine,
  title = {A Framework for Mesencephalic Dopamine Systems Based on Predictive {{Hebbian}} Learning. - {{PubMed}} - {{NCBI}}},
  howpublished = {https://www.ncbi.nlm.nih.gov/pubmed/8774460},
  file = {/home/mo/Zotero/storage/5H76NCY5/8774460.html}
}

@misc{ModelbasedInfluencesHumans,
  title = {Model-Based Influences on Humans' Choices and Striatal Prediction Errors},
  howpublished = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3077926/},
  file = {/home/mo/Zotero/storage/K2ZUT6TT/PMC3077926.html}
}

@article{montagueFrameworkMesencephalicDopamine1996,
  title = {A Framework for Mesencephalic Dopamine Systems Based on Predictive {{Hebbian}} Learning},
  volume = {16},
  copyright = {\textcopyright{} 1996 by Society for Neuroscience},
  issn = {0270-6474, 1529-2401},
  abstract = {We develop a theoretical framework that shows how mesencephalic dopamine systems could distribute to their targets a signal that represents information about future expectations. In particular, we show how activity in the cerebral cortex can make predictions about future receipt of reward and how fluctuations in the activity levels of neurons in diffuse dopamine systems above and below baseline levels would represent errors in these predictions that are delivered to cortical and subcortical targets. We present a model for how such errors could be constructed in a real brain that is consistent with physiological results for a subset of dopaminergic neurons located in the ventral tegmental area and surrounding dopaminergic neurons. The theory also makes testable predictions about human choice behavior on a simple decision-making task. Furthermore, we show that, through a simple influence on synaptic plasticity, fluctuations in dopamine release can act to change the predictions in an appropriate manner.},
  language = {en},
  number = {5},
  journal = {Journal of Neuroscience},
  doi = {10.1523/JNEUROSCI.16-05-01936.1996},
  author = {Montague, P. R. and Dayan, P. and Sejnowski, T. J.},
  month = mar,
  year = {1996},
  pages = {1936-1947},
  file = {/home/mo/Zotero/storage/GQUNAJ5K/Montague et al. - 1996 - A framework for mesencephalic dopamine systems bas.pdf;/home/mo/Zotero/storage/7H67ZY5F/1936.html},
  pmid = {8774460}
}

@article{dawModelbasedInfluencesHumans2011,
  title = {Model-Based Influences on Humans' Choices and Striatal Prediction Errors},
  volume = {69},
  issn = {0896-6273},
  abstract = {The mesostriatal dopamine system is prominently implicated in model-free
reinforcement learning, with fMRI BOLD signals in ventral striatum notably
covarying with model-free prediction errors. However, latent learning and
devaluation studies show that behavior also shows hallmarks of model-based
planning, and the interaction between model-based and model-free values,
prediction errors and preferences is underexplored. We designed a multistep
decision task in which model-based and model-free influences on human choice
behavior could be distinguished. By showing that choices reflected both
influences we could then test the purity of the ventral striatal BOLD signal as
a model-free report. Contrary to expectations, the signal reflected both
model-free and model-based predictions in proportions matching those that best
explained choice behavior. These results challenge the notion of a separate
model-free learner and suggest a more integrated computational architecture for
high-level human decision-making.},
  number = {6},
  journal = {Neuron},
  doi = {10.1016/j.neuron.2011.02.027},
  author = {Daw, Nathaniel D. and Gershman, Samuel J. and Seymour, Ben and Dayan, Peter and Dolan, Raymond J.},
  month = mar,
  year = {2011},
  pages = {1204-1215},
  file = {/home/mo/Zotero/storage/3F4WB9ET/Daw et al. - 2011 - Model-based influences on humans choices and stri.pdf},
  pmid = {21435563},
  pmcid = {PMC3077926}
}


